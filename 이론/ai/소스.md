PEFT
- Unsloth
    - LLM을 효율적으로 미세조정(Fine-tuning)하기 위한 라이브러리
    - 수작업으로 유도된 수학 연산과 GPU 커널을 직접 작성할 수 있음
    - 오픈 소스, 무료
    
- LoRA
    - Low-Rank Adaptation
    - LLM을 효율적으로 파인튜닝하는 기술
    - 적은 수의 추가적인 매개변수만 학습시켜 모델의 성능을 향상시킴
- LoRA - Adapter
    - 기존에 모델을 구성하고 있던 계층과는 별개의 파라미터를 사용하는 개념
    - forward 연산은 pretrained weights와 adapter를 따로 수행하고, 각 결과를 합쳐서 다음 레이어에 보냄
    - backward 연산은 pretrained weights가 필요하지 않고, adapter는 파라미터 수가 굉장히 적어서 가능
    - but 이렇게 하면, forward 추론 시 불리하기에, 학습이 끝난 adapter의 가중치를 pretrained weights에 더하면서 해결
- PEFT
    - Parameter-Efficient Fine-Tuning
    - 적은 수의 매개변수만 학습시켜 LLM을 효율적으로 파인튜닝하는 기술들의 집합
- 학습 로직
    - 기존 학습/추론 로직
        - 데이터 로드 → 데이터셋 클래스 만들기 → 모델, 학습 args 불러오기 → Trainer로 ‘데이터, 모델, args’ 감싸기 → Trainer.train()
    - LoRA 추가 시 학습/추론 로직
        - 데이터 로드 → 데이터셋 클래스 만들기 → 모델, 학습 args 불러오기 → **PEFT(LoRA)** config 설정하기 → Trainer로 **‘데이터, 모델, args, PEFT config’** 감싸기 → Trainer.train()
- LoRA 설정
    
    ```python
    from peft import LoraConfig # LoraConfig 클래스를 가져옵니다.
    
    # LoraConfig 객체는 LoRA 파인튜닝에 필요한 설정을 정의합니다.
    peft_config = LoraConfig(
        # r: LoRA의 랭크(rank)를 의미합니다.
        # r 값이 작을수록 학습할 매개변수가 적어 메모리 사용량이 줄지만,
        # 모델 성능이 낮아질 수 있습니다.
        # r 값이 클수록 성능은 좋아지지만, 메모리 사용량과 학습 시간이 늘어납니다.
        r=args.lora_r, 
    
        # lora_alpha: LoRA 스케일링 계수입니다.
        # LoRA 레이어의 출력을 조정하는 역할로, 일반적으로 r 값에 비례하여 설정합니다 (예: 2*r).
        # 값이 클수록 LoRA의 영향력이 커집니다.
        **lora_alpha**=args.lora_alpha,
    
        # lora_dropout: LoRA 레이어에 적용되는 드롭아웃 비율입니다.
        # 과적합(overfitting)을 방지하기 위해 사용됩니다.
        lora_dropout=args.lora_dropout,
    
        # bias: LoRA를 적용할 레이어의 편향(bias)을 학습할지 여부를 설정합니다.
        # 'none' (기본값)은 학습하지 않고, 'lora_only'는 LoRA 레이어의 편향만,
        # 'all'은 모든 편향을 학습합니다.
        bias=args.lora_bias, 
    
        # task_type: 파인튜닝할 작업의 유형을 지정합니다.
        # 예: "CAUSAL_LM" (텍스트 생성), "SEQ_CLS" (텍스트 분류) 등.
        # 올바른 모델 아키텍처를 선택하는 데 사용됩니다.
        task_type=args.task_type,
    
        # target_modules: LoRA를 적용할 모델의 특정 모듈(레이어)을 지정합니다.
        # 일반적으로 Transformer 모델의 어텐션(attention) 레이어인 'q_proj', 'k_proj', 'v_proj' 등을 지정합니다.
        **target_modules**=args.target_modules 
    )
    
    # Trainer 객체를 생성하여 모델 학습을 준비합니다.
    trainer = MyTrainer(
        model=model, # 학습할 모델
        args=training_args, # 학습에 필요한 인자들 (학습률, 에포크 수 등)
        train_dataset=train_dataset, # 학습 데이터셋
        eval_dataset=valid_dataset, # 검증 데이터셋
        # peft_config: PEFT 설정을 Trainer에 전달합니다.
        # args.peft가 True인 경우에만 LoRA 설정을 적용합니다.
        peft_config=peft_config if args.peft else None, 
    )
    ```
    
- LoRA - target_modules
    - 각 속성에 들어갈 수 있는 속성들
    - Attention: q_proj, k_proj, v_proj, o_proj
    - MLP: gate_proj, up_proj, down_proj
- 대화형 데이터를 학습에 적합한 형식으로 변환하는 과정
    
    ```jsx
    def formatting_prompts_func(examples):
       convos = examples["conversations"]
       texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for convo in convos]
       return { "text" : texts, }
    
    dataset = dataset.map(formatting_prompts_func, batched = True)
    ```
    
    - tokenize=False
        - 대화를 토큰 ID로 변환하지 않고, 모델이 이해할 수 있는 문자열 형식으로만 변환
    - add_generation_prompt=False, 모델이 응답을 생성할 준비가 되었음을 나타내는 <|assistant|>와 같은 추가 프롬프트 토큰을 제거
    - 주의할 점
        - `assistant`는 정답이기 때문에 학습에서만 입력으로 넣어주고 추론에서는 넣으면 안됨
        - `add_generation_prompt=False`는 `assistant`가 들어갔기 때문에 `False`로 설정
            - 추론용이라면 `True`로 설정
- SFT(Supervised Fine-Tuning)
    - 특정 작업을 수행하도록 미리 학습된 대규모 언어 모델(LLM)을 학습시키는 방법
    - 속성
        - 전체 학습을 원한다면 num_train_epochs=1, max_steps=None
        - 속성 코드
            
            ```python
            trainer = SFTTrainer(
                model = model,
                tokenizer = tokenizer,
                train_dataset = dataset,
                args = SFTConfig(
            		    # 학습에 사용할 텍스트가 담긴 열 
                    dataset_text_field="text",
                    
                    # 하나의 GPT당 훈련 배치 크기 
                    # 배치 크기가 크면 학습이 안정적일 수 있지만, 많은 메모리 필요
                    per_device_train_batch_size = 8,
                    
                    # 기울기를 축적하는 단계 수
                    # 기울기를 축적하면 배치 크기가 더 큰 것처럼 학습 가능
                    # GPU 메모리가 부족해 배치 크기를 키우지 못할 때 유용 
                    # ex) 4로 설정하면, 8개짜리 배치를 4번 처리한 기울기를 누적했다가
                    # 한번에 없데이트 
                    gradient_accumulation_steps = 1,
                    
                    # 학습률을 점진적으로 증가시키는 단계 수 
                    warmup_steps = 5,
                    
                    # 전체 학습에 사용할 최대 업데이트 단계 수
                    # 클수록 더 오래 학습 진행 
                    max_steps = 100,
                    
                    # 학습의 속도를 결정하는 핵심 파라미터
                    # 크면 최적점을 지나치고, 작으면 학습 속도가 느려짐
                    learning_rate = 5e-5,
                    
            				# 몇 단계마다 로그를 기록할지 결정
                    logging_steps = 1,
                    
                    # 최적화 도구를 지정 
                    optim = "adamw_8bit",
                    
                    # 가중치 감쇠(L2 정규화) 값 - 과적합 방지
                    weight_decay = 0.01,
                    
                    # 학습률 스케쥴러의 종류를 지정 
                    # linear는 학습 초기에 학습률을 서서히 높였다가 점진적으로 낮추는 방식
                    lr_scheduler_type = "linear",
                    
                    # 모델 체크포인트와 로그 파일이 저장될 디렉토리
                    output_dir = "outputs",
                    
                    # 학습 진행 상황을 보고할 플랫폼 (ex: wandb)
                    report_to = "none",
                ),
            )
            ```
            
- Unsloth - train_on_responses_only
    - User Input에 대한 loss는 무시하고 오직 assistant(정답 라벨) output만 학습
    - 즉, 사용자 질문이 아닌 응답에서만 학습하라는 의미
    - 코드
    
    ```python
    trainer = train_on_responses_only(
        trainer,
        instruction_part = instruction_part,
        response_part = response_part,
    )
    ```
    
- transformers 라이브러리를 사용하여 LLM이 텍스트를 생성하는 과정
    
    ```python
    from transformers import TextStreamer
    _ = model.generate(
    		# 입력 텍스트를 토큰화 
        **tokenizer(text, return_tensors = "pt").to("cuda"),
        max_new_tokens = 125,
        # temperature: 1에 가까울수록 무작위적이고 창의적
        # top_p: 핵심 샘플링을 위한 파라미터, 높으면 예상 가능한 답변
        # top_k : Top-k 샘플링을 위한 파라미터, 고려할 후보 개수 
        temperature = 1, top_p = 0.95, top_k = 64,
        # 모델의 답변을 실시간으로 스트리밍하는 기능을 활성화 
        # skip_prompt: 모델이 생성하는 새로운 텍스트만 화면에 보여줌 
        streamer = TextStreamer(tokenizer, skip_prompt = True),
    )
    ```
    
- NLP에서 중요하게 체크해야 할 EDA 항목
    1. sequence length
    2. langauge
    3. input data domain
- pad_token
    - 데이터를 효율적으로 처리하기 위해서 “가장 긴 문자의 길이에 맞춰 나머지 짧은 문장의 뒤에 채워 넣는 역할”
- eos_token
    - 문장이 어디서 끝나는지 보여주는 토큰
- 양자화(Quantization)
    - 모델의 매개변수를 더 낮은 정밀도로 표현하여 모델의 크기와 연산량을 줄이는 기술
    - 연산/메모리 부하를 줄이는 가장 직관적인 방법
- PTQ(Post-Traning Quantization)
    - 모델 학습이 완료된 후, 모델의 가중치와 화설호아 함수 값을 낮은 정밀도로 변환
    - 재학습 과정이 필요 없어서 매우 간단하지만 정확도 손실이 발생할 수 있음
- QAT(Quantization-Aware Training)
    - **모델 학습 과정**에 양자화를 추가시켜 모델이 낮은 정밀도에 적응하도록 훈련하는 방식
    - 정확도 손실이 적지만, 복잡하고 학습데이터가 필요함
- weight-only quantization vs weight-activation quantization
    - weight-only quantization(가중치 전용 양자화)
        - 가중치만 양자화하는 방식
        - 모델 크기의 대부분을 차지하는 가중치를 압축하기 때문에 공간을 크게 아낄 수 있음
        - 활성화 값은 여전히 높은 정밀로 계싼되기 때문에 추론 속도나 사용량을 개선하진 못함
    - weight-activation quantization(가중치 활성화 양자화)
        - 가중치와 활성화 값 모두를 낮은 정밀도로 양자화하는 방법
        - 모델 크기를 최대로 줄이고, 효율을 올릴 수 있음
        - 복잡하고, 모델 정확도 손실이 더 클 수 있음
- Symmetric vs Asymmetric
    - Symmetric(대칭)
        - 원점을 중심으로 양자화 범위를 설정, 최대/최소 절대값이 같도록 범위를 설정
        - 간단하지만, 데이터 분포가 비대칭적일 경우, 사용되지 않는 양자화 범위가 생겨서 정확도 손실이 생길 수 있음
    - Asymmetric(비대칭)
        - 실제 데이터의 최솟값과 최댓값에 맞춰 양자화 범위를 설정
        - 정확도 손실이 적지만, zero-point(원점)을 따로 관리해야 해서 구현이 까다로움
- Mixed-precision quantization
    - 모델의 각기 다른 부분에서 서로 다른 정밀도를 적용하는 양자화 방식
    - 효율적이고, 높은 유연성이 있음
    - 복잡하고, 하드웨어 제약이 있을 수 있음
- 양자화 - Pruning
    - 가지치기
    - 모델의 가중치 중 중요도가 낮은 연결을 제거하여 모델을 작게 만드는 기술
    - magnitude-based pruning
        - 신경망 가중치의 절대값 크기를 기준으로 중요도가 낮은 가중치를 제거
        - 가장 간단하고 일반적인 가지치기 기법
- 양자화 - Unstructured vs Structured
    - Unstructured(비정형)
        - 모델의 구조와 관계없이 개별 가중치를 제거하는 방식
        - 정확도 손실 없이 많은 가중치를 제거할 수 있음
        - 모델의 연결이 불규칙적으로 끊어지기에, 효율적인 연산이 어려움
    - Structured Pruning(정형)
        - 모델의 구조(채널, 필터, 행, 열) 단위로 가중치를 제거하는 방식
        - 효율적이지만, 모델 압축률이 낮을 수 있음
- 가지치기 인덱싱 데이터 형식
    - 불필요한 0의 가중치들을 제외하고, 유효한 값들만 저장하는 방식
    - COO(Coordinate)
        - 0이 아닌 각 원소를 (행 좌표, 열 좌표)의 형태로 저장
        - 직관적이고, 쉽게 추가/삭제가 가능
        - 인덱스 정보를 모두 저장해야 해서 메모리 사용량이 큼
    - CSR(Compressed Sparse Row) | CSC(Compressed Sparse Column)
        - 행을 기준으로 압축
        - value: 0이 아닌 값들을 순서대로 저장
        - column_indices: 각 값의 열 인덱스를 저장
        - row_pointers: 각 행의 시작 위치를 values 배열의 인덱스로 저장
        - 행 단위로 접근하기 쉽고, 벡터 곱셈 연산에 효율적
        - 특정 행의 모든 값을 읽거나 행을 삽입/삭제하는 것이 복잡함
- 지식 증류 (Knowledge Distillation)
    - **크고 똑똑한 교사 모델**의 풍부한 학습 노하우를 **작고 빠른 학생 모델**에게 전수하여, 성능은 유지하면서도 모델의 크기와 연산량을 줄이는 기술
- PEFT
    - Parameter-Efficient Fine-Tuning
    - 적은 수의 매개변수만 학습시켜 LLM을 효율적으로 파인튜닝하는 기술들의 집합
    - LoRA
- LoRA
    - Low-Rank Adaptation
    - LLM을 효율적으로 파인튜닝하는 기술
    - 적은 수의 추가적인 매개변수만 학습시켜 모델의 성능을 향상시킴
    - 기존 가중치 고정 → 새로운 작은 행렬 추가 → 미세 조정 → 추론
- LoRA - Adapter
    - 기존에 모델을 구성하고 있던 계층과는 별개의 파라미터를 사용하는 개념
    - forward 연산은 pretrained weights와 adapter를 따로 수행하고, 각 결과를 합쳐서 다음 레이어에 보냄
    - backward 연산은 pretrained weights가 필요하지 않고, adapter는 파라미터 수가 굉장히 적어서 가능
    - but 이렇게 하면, forward 추론 시 분리하기에, 학습이 끝난 adapter의 가중치를 pretrained weights에 더하면서 해결
- QLoRA
    - 경량화 + LoRA
    - 원본 모델을 강하게 양자화 → 범용 성능의 하락
    - LoRA의 작은 가중치 행렬을 학습
    - 획기적인 메모리 절감, 높은 성능, 비용 횽류성, 접근성 향상