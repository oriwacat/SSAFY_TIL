## 문제 1 (난이도: 쉬움)
LLM(거대 언어 모델)의 전체 가중치를 모두 업데이트하는 대신, 기존 가중치는 고정한 채 소수의 추가적인 매개변수만 학습시켜 계산 자원과 시간을 크게 절약하는 파인튜닝 기술들을 총칭하는 용어는 무엇인가?

### 보기
A. 전이 학습 (Transfer Learning)
B. 파라미터 효율적 파인튜닝 (PEFT, Parameter-Efficient Fine-Tuning)
C. 강화 학습 (Reinforcement Learning)
D. 비지도 학습 (Unsupervised Learning)
E. 전체 파인튜닝 (Full Fine-Tuning)

**정답:** B
**해설:** PEFT는 수십억 개의 파라미터를 가진 거대 모델을 특정 작업에 맞게 조정할 때, 모든 파라미터를 업데이트하는 대신 일부(보통 1% 미만)의 파라미터만 효율적으로 학습하는 방법론들을 의미합니다. LoRA, Prefix-Tuning, Adapter 등 다양한 기법이 여기에 속합니다.
- A: 미리 학습된 모델을 다른 작업에 사용하는 더 넓은 개념입니다.
- E: PEFT와 대비되는 개념으로, 모델의 모든 가중치를 학습시키는 방식입니다.
- C, D: 다른 종류의 머신러닝 패러다임입니다.

---

## 문제 2 (난이도: 보통)
PEFT 기법 중 가장 널리 사용되는 LoRA(Low-Rank Adaptation)의 핵심 아이디어로 가장 적절한 것은?

### 보기
A. 모델의 전체 레이어를 새로운 작은 레이어로 교체하여 학습한다.
B. 기존의 사전 학습된 가중치(W)는 고정시키고, 그 옆에 랭크(rank)가 낮은 두 개의 작은 행렬(A, B)을 추가하여 이 행렬들만 학습시킨다.
C. 모델의 가중치를 낮은 정밀도(예: 4비트)로 양자화하여 메모리 사용량을 줄인다.
D. 모델의 가중치 중 중요도가 낮은 연결을 제거(pruning)하여 모델을 경량화한다.
E. 교사 모델의 출력을 학생 모델이 모방하도록 학습시킨다.

**정답:** B
**해설:** LoRA는 파인튜닝 시 발생하는 가중치의 변화량(ΔW)이 낮은 랭크(low-rank) 특성을 가질 것이라는 가정에서 출발합니다. 따라서 거대한 ΔW 전체를 학습하는 대신, 이를 두 개의 작은 행렬(A, B)의 곱(ΔW ≈ BA)으로 근사하여 A와 B 행렬만 학습합니다. 이를 통해 학습 대상 파라미터 수를 획기적으로 줄일 수 있습니다.
- C: 양자화(Quantization)에 대한 설명입니다.
- D: 가지치기(Pruning)에 대한 설명입니다.
- E: 지식 증류(Knowledge Distillation)에 대한 설명입니다.

---

## 문제 3 (난이도: 보통)
LoRA 설정에서, 학습할 저-랭크 행렬의 크기를 결정하는 가장 중요한 하이퍼파라미터는 무엇인가? (이 값이 클수록 표현력은 높아지지만, 학습할 파라미터 수도 증가한다.)

### 보기
A. `lora_alpha`
B. `lora_dropout`
C. `r` (rank)
D. `target_modules`
E. `task_type`

**정답:** C
**해설:** `r` (rank)은 LoRA에서 추가되는 저-랭크 행렬의 차원을 결정하는 핵심 하이퍼파라미터입니다. 예를 들어, 가중치 행렬 A는 d x r, B는 r x k 차원을 가지게 되어, `r` 값이 학습 파라미터의 수와 모델의 표현력을 직접적으로 조절합니다. 일반적으로 8, 16, 32, 64 등의 값을 사용합니다.
- A: LoRA의 스케일링 계수입니다.
- B: 과적합 방지를 위한 드롭아웃 비율입니다.
- D: LoRA를 적용할 모델 내의 특정 레이어를 지정합니다.
- E: 파인튜닝할 작업의 종류를 명시합니다.

---

## 문제 4 (난이도: 보통)
LoRA를 적용할 때, `target_modules` 파라미터를 사용하여 어느 레이어에 LoRA 행렬을 추가할지 지정한다. 일반적으로 트랜스포머 기반 모델에서 가장 효과적이라고 알려져 가장 흔하게 지정되는 모듈은 무엇인가?

### 보기
A. 입력 임베딩 레이어 (Input Embedding Layer)
B. 최종 출력 레이어 (Final Output Layer)
C. 어텐션 레이어의 쿼리/밸류 프로젝션 (q_proj, v_proj)
D. 레이어 정규화 (Layer Normalization)
E. 활성화 함수 (Activation Functions)

**정답:** C
**해설:** 연구에 따르면 트랜스포머 모델의 여러 구성 요소 중, 셀프 어텐션(Self-Attention) 블록 내의 쿼리(Query)와 밸류(Value) 프로젝션 레이어(`q_proj`, `v_proj`)에 LoRA를 적용하는 것이 가장 파라미터 효율적이면서 높은 성능을 보인다고 알려져 있습니다. 많은 경우 `k_proj`, `o_proj`나 MLP 레이어에도 적용하기도 합니다.
- 다른 레이어들에 적용하는 것보다 어텐션 관련 레이어에 적용하는 것이 일반적이고 효과적입니다.

---

## 문제 5 (난이도: 보통)
모델의 가중치와 활성화 값을 더 낮은 정밀도(예: 32비트 부동소수점 → 8비트 정수)로 표현하여, 모델 크기와 메모리 사용량을 줄이고 추론 속도를 향상시키는 모델 최적화 기술은 무엇인가?

### 보기
A. 가지치기 (Pruning)
B. 지식 증류 (Knowledge Distillation)
C. 양자화 (Quantization)
D. 믹스드 프리시전 (Mixed Precision)
E. 정규화 (Normalization)

**정답:** C
**해설:** 양자화(Quantization)는 모델의 파라미터를 표현하는 데 사용되는 비트 수를 줄이는 기술입니다. 예를 들어, 32비트로 표현되던 하나의 가중치를 8비트나 4비트로 표현하면, 모델의 크기는 1/4 또는 1/8로 줄어들고, 정수 연산을 통해 추론 속도도 빨라질 수 있습니다. 이는 모델을 경량화하는 가장 대표적인 방법 중 하나입니다.
- A: 중요도 낮은 가중치를 제거하는 기술입니다.
- B: 큰 모델의 지식을 작은 모델로 전달하는 기술입니다.
- D: 학습 시 안정성과 속도를 위해 16비트와 32비트를 함께 사용하는 기술입니다.
- E: 데이터의 분포를 조정하는 전처리 기술입니다.

---

## 문제 6 (난이도: 보통)
양자화(Quantization) 기법 중, 이미 학습이 완료된 모델에 대해 재학습 없이 가중치를 낮은 정밀도로 변환하는 방식은 무엇인가? (이 방식은 적용이 간단하지만 정확도 손실이 발생할 수 있다.)

### 보기
A. QAT (Quantization-Aware Training)
B. PTQ (Post-Training Quantization)
C. 가중치 전용 양자화 (Weight-only Quantization)
D. 비대칭 양자화 (Asymmetric Quantization)
E. 혼합 정밀도 양자화 (Mixed-precision Quantization)

**정답:** B
**해설:** PTQ(학습 후 양자화)는 이름 그대로, 이미 모든 학습이 끝난 모델을 가져와 양자화만 수행하는 방식입니다. 별도의 학습 데이터나 복잡한 파이프라인이 필요 없어 매우 빠르고 간단하게 적용할 수 있다는 장점이 있지만, 모델이 양자화로 인한 오차에 적응할 기회가 없었기 때문에 QAT에 비해 정확도 하락이 더 클 수 있습니다.
- A: 학습 과정 중에 양자화를 시뮬레이션하여 정확도 손실을 최소화하는 방식입니다.
- C, D, E는 양자화를 다른 기준으로 분류한 것입니다.

---

## 문제 7 (난이도: 보통)
모델의 가중치 중 중요도가 낮다고 판단되는 연결(파라미터)을 영구적으로 제거하여 모델을 경량화하고, 희소(sparse)한 모델로 만드는 기술은 무엇인가?

### 보기
A. 드롭아웃 (Dropout)
B. 가중치 감쇠 (Weight Decay)
C. 양자화 (Quantization)
D. 가지치기 (Pruning)
E. 지식 증류 (Knowledge Distillation)

**정답:** D
**해설:** 가지치기(Pruning)는 마치 나뭇가지를 치는 것처럼, 모델의 성능에 거의 영향을 주지 않는 작은 값의 가중치들을 0으로 만들거나 아예 제거하는 기술입니다. 이를 통해 모델의 파라미터 수를 줄여 크기를 줄이고, 특정 하드웨어에서는 추론 속도를 높일 수 있습니다.
- A: 학습 시에만 무작위로 뉴런을 비활성화하는 규제 기법으로, 추론 시에는 모든 연결이 복원됩니다.
- B: 가중치 값이 너무 커지지 않도록 제약을 거는 L2 규제 기법입니다.
- C: 가중치의 정밀도를 낮추는 기술입니다.
- E: 큰 모델의 지식을 작은 모델로 전달하는 기술입니다.

---

## 문제 8 (난이도: 보통)
가지치기(Pruning) 기법 중, 모델의 특정 구조(채널, 필터, 행 등)와 관계없이 개별 가중치를 독립적으로 제거하는 방식은 무엇인가? (이 방식은 압축률이 높지만, 비정형적인 희소성으로 인해 하드웨어 가속이 어려울 수 있다.)

### 보기
A. 정형 가지치기 (Structured Pruning)
B. 비정형 가지치기 (Unstructured Pruning)
C. 크기 기반 가지치기 (Magnitude-based Pruning)
D. 동적 가지치기 (Dynamic Pruning)
E. 점진적 가지치기 (Gradual Pruning)

**정답:** B
**해설:** 비정형 가지치기는 모델 전체에서 가중치 값의 크기 등 기준에 따라 중요도가 낮은 개별 파라미터들을 제거합니다. 이 방식은 모델 구조를 유지해야 하는 제약이 없어 높은 압축률을 달성할 수 있지만, 결과적으로 생성되는 희소 행렬의 패턴이 불규칙하여 일반적인 GPU 등에서 연산 가속을 얻기 어렵다는 단점이 있습니다.
- A: 채널이나 필터 등 구조적인 단위를 통째로 제거하여 하드웨어 가속에 유리한 방식입니다.
- C는 가지치기 대상을 선정하는 기준 중 하나이며, 비정형/정형 방식 모두에 적용될 수 있습니다.

---

## 문제 9 (난이도: 보통)
크고 복잡한 '교사 모델'의 예측 결과(soft label)를 작고 빠른 '학생 모델'이 모방하도록 학습시켜, 모델의 크기는 줄이면서도 성능은 최대한 보존하는 모델 압축 기술은 무엇인가?

### 보기
A. 전이 학습 (Transfer Learning)
B. 메타 학습 (Meta-Learning)
C. 지식 증류 (Knowledge Distillation)
D. 자기 지도 학습 (Self-supervised Learning)
E. 앙상블 학습 (Ensemble Learning)

**정답:** C
**해설:** 지식 증류는 잘 학습된 대형 모델(교사)이 가진 '지식'을 경량 모델(학생)에게 전달하는 기법입니다. 교사 모델이 출력하는 클래스별 확률 분포(soft label)에는 정답 레이블(hard label)에는 없는 클래스 간의 미묘한 관계 정보가 담겨 있는데, 학생 모델이 이를 학습함으로써 더 풍부한 정보를 바탕으로 효율적인 학습이 가능해집니다.
- A, B, D, E는 다른 목적을 가진 학습 방법론입니다.

---

## 문제 10 (난이도: 보통)
LoRA와 4비트 양자화를 결합하여, GPU 메모리 사용량을 획기적으로 줄이면서도 준수한 성능을 유지하여 일반 소비자용 GPU에서도 거대 언어 모델의 파인튜닝을 가능하게 만든 기술은 무엇인가?

### 보기
A. LoRA
B. QLoRA
C. AdaLoRA
D. LongLoRA
E. Unsloth

**정답:** B
**해설:** QLoRA(Quantized Low-Rank Adaptation)는 LoRA를 한 단계 더 발전시킨 기술입니다. 사전 학습된 모델의 전체 가중치를 4비트로 양자화하여 메모리에 로드하고, 이 양자화된 모델에 LoRA 모듈을 추가하여 파인튜닝을 진행합니다. 이를 통해 16비트로 전체 모델을 로드해야 했던 기존 방식에 비해 메모리 요구량을 약 1/4로 줄여, 훨씬 적은 자원으로도 LLM 파인튜닝을 가능하게 했습니다.
- A: QLoRA의 기반이 되는 기술입니다.
- C, D: LoRA의 다른 변형 기술들입니다.
- E: LoRA, QLoRA 등의 학습을 더 빠르게 최적화해주는 라이브러리입니다.

---

## 문제 11 (난이도: 보통)
SFTTrainer와 같은 LLM 파인튜닝 라이브러리에서, GPU 메모리가 부족하여 배치 크기(batch size)를 키울 수 없을 때, 여러 스텝에 걸쳐 계산된 기울기(gradient)를 누적했다가 한 번에 모델 가중치를 업데이트하여 큰 배치 크기로 학습하는 것과 유사한 효과를 내는 파라미터는 무엇인가?

### 보기
A. `learning_rate`
B. `warmup_steps`
C. `max_steps`
D. `gradient_accumulation_steps`
E. `weight_decay`

**정답:** D
**해설:** `gradient_accumulation_steps`는 작은 배치로 여러 번 forward/backward를 수행하여 계산된 기울기를 메모리에 계속 더해 나간 뒤, 지정된 스텝 수만큼 누적되면 한 번에 옵티마이저를 통해 가중치를 업데이트하는 방식입니다. 예를 들어, 배치 크기가 8이고 이 값을 4로 설정하면, 실질적으로는 배치 크기 32(8 * 4)로 학습하는 것과 유사한 효과를 얻을 수 있습니다.
- A, B, C, E는 학습률, 웜업, 최대 스텝, 가중치 감쇠 등 다른 중요한 학습 관련 하이퍼파라미터입니다.

---

## 문제 12 (난이도: 보통)
양자화(Quantization) 기법 중, 모델의 학습(training) 과정에 양자화로 인한 오차를 미리 시뮬레이션하여, 모델이 낮은 정밀도에 적응하도록 함께 훈련하는 방식은 무엇인가? (이 방식은 복잡하지만 정확도 손실을 최소화할 수 있다.)

### 보기
A. PTQ (Post-Training Quantization)
B. QAT (Quantization-Aware Training)
C. 동적 양자화 (Dynamic Quantization)
D. 정적 양자화 (Static Quantization)
E. 가중치 전용 양자화 (Weight-only Quantization)

**정답:** B
**해설:** QAT(양자화 인지 학습)는 파인튜닝 또는 초기 학습 단계에서부터 모델의 가중치가 양자화될 것을 '인지'시키며 학습을 진행합니다. 순전파 시에는 양자화를 시뮬레이션하고, 역전파 시에는 실제 가중치를 업데이트함으로써, 모델이 양자화로 인해 발생하는 미세한 오차에 강건해지도록 만듭니다. 이로 인해 PTQ보다 높은 정확도를 유지할 수 있습니다.
- A: 학습이 끝난 후 양자화를 적용하는 방식입니다.
- C, D는 양자화 적용 시점에 따른 다른 분류입니다.

---

## 문제 13 (난이도: 보통)
희소 행렬(Sparse Matrix)을 효율적으로 저장하는 데이터 형식 중, 0이 아닌 각 원소의 (행 좌표, 열 좌표, 값)을 튜플 형태로 저장하는 가장 직관적인 방식은 무엇인가?

### 보기
A. CSR (Compressed Sparse Row)
B. CSC (Compressed Sparse Column)
C. COO (Coordinate)
D. DOK (Dictionary of Keys)
E. LIL (List of Lists)

**정답:** C
**해설:** COO 형식은 0이 아닌 모든 원소에 대해 (행 인덱스, 열 인덱스, 값)이라는 3개의 정보를 리스트 형태로 저장하는 방식입니다. 구조가 매우 단순하고 직관적이며, 원소 추가가 쉽다는 장점이 있지만, 인덱스 정보를 모두 저장해야 하므로 다른 압축 형식에 비해 메모리 사용량이 클 수 있습니다.
- A, B는 행 또는 열을 기준으로 인덱스를 압축하여 COO보다 메모리 효율성이 높고 특정 연산에 유리한 형식입니다.

---

## 문제 14 (난이도: 보통)
대화형 데이터셋을 모델 학습 형식에 맞게 변환할 때, `tokenizer.apply_chat_template` 함수의 `add_generation_prompt=False`로 설정하는 경우는 주로 어떤 상황인가?

### 보기
A. 모델이 생성해야 할 다음 차례임을 알려주는 프롬프트를 추가하고 싶을 때
B. 학습 데이터셋에서, 이미 'assistant'의 응답(정답)까지 모두 포함된 전체 대화 내용을 입력으로 사용할 때
C. 추론(inference) 시, 사용자 입력까지만 주고 모델의 응답 생성을 유도할 때
D. 토크나이저가 특수 토큰을 자동으로 추가하는 것을 막고 싶을 때
E. 모델의 최대 컨텍스트 길이를 초과하는 긴 대화를 처리할 때

**정답:** B
**해설:** `add_generation_prompt`는 모델이 응답을 생성할 차례임을 알려주는 프롬프트(예: `<|assistant|>`)를 문자열 끝에 추가할지 여부를 결정합니다. 학습 시에는 이미 정답에 해당하는 `assistant`의 응답까지 텍스트에 포함되어 있으므로, 또 다른 생성 프롬프트를 추가할 필요가 없습니다. 따라서 이 경우 `False`로 설정합니다. 반대로, 추론 시에는 사용자 입력까지만 제공하고 모델의 생성을 유도해야 하므로 `True`로 설정하는 것이 일반적입니다.
- A, C는 `True`로 설정해야 하는 경우입니다.

---

## 문제 15 (난이도: 보통)
Unsloth와 같은 최적화 라이브러리를 사용하여 SFT(Supervised Fine-Tuning)를 진행할 때, 데이터셋에서 사용자의 질문(instruction) 부분에 대한 손실(loss)은 계산하지 않고, 오직 모델의 응답(response) 부분에 대해서만 학습을 진행하도록 하는 설정의 주된 목적은 무엇인가?

### 보기
A. 학습 시간을 절반으로 줄이기 위해
B. 모델이 질문을 생성하는 능력을 학습하는 것을 방지하고, 주어진 질문에 대한 올바른 응답을 생성하는 데만 집중하도록 하기 위해
C. GPU 메모리 사용량을 줄이기 위해
D. 모델이 더 창의적이고 다양한 질문을 생성하도록 유도하기 위해
E. 사용자의 질문에 있는 잠재적인 오류나 노이즈가 모델 학습에 영향을 주는 것을 막기 위해

**정답:** B
**해설:** 대화형 파인튜닝의 목표는 모델이 주어진 지시나 질문에 대해 적절한 '응답'을 생성하도록 학습시키는 것입니다. 사용자의 질문은 이미 주어진 입력(context)이므로, 모델이 이 부분을 예측하도록 학습할 필요가 없습니다. 따라서 질문 부분의 손실은 무시하고 오직 모델이 생성해야 할 정답인 응답 부분의 손실만을 계산하여 역전파함으로써, 학습 목표에 더 집중하고 효율성을 높일 수 있습니다.
