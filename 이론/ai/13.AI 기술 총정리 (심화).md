## 문제 1 (난이도: 쉬움)
다음 머신러닝 학습 방식 중, 데이터에 정답(label)이 주어져 모델이 입력과 정답 사이의 관계를 학습하는 것은 무엇인가?

### 보기
A. 비지도 학습 (Unsupervised Learning)
B. 강화 학습 (Reinforcement Learning)
C. 지도 학습 (Supervised Learning)
D. 자기 지도 학습 (Self-supervised Learning)
E. 제로샷 학습 (Zero-shot Learning)

**정답:** C
**해설:** 지도 학습은 '정답지'가 있는 데이터를 사용하여 모델을 학습시키는 방법입니다. 모델은 주어진 입력에 대해 올바른 출력을 내는 방법을 배우며, 회귀와 분류가 대표적인 예입니다.
- A: 정답 없는 데이터에서 숨겨진 구조나 패턴을 학습합니다.
- B: 에이전트가 환경과 상호작용하며 보상을 최대화하는 행동을 학습합니다.
- D: 데이터 자체의 일부를 정답으로 삼아 학습하는 방식입니다.

---

## 문제 2 (난이도: 쉬움)
연속적인 숫자 값(예: 주택 가격, 내일의 온도)을 예측하는 지도 학습의 한 종류는 무엇인가?

### 보기
A. 분류 (Classification)
B. 회귀 (Regression)
C. 군집화 (Clustering)
D. 차원 축소 (Dimensionality Reduction)
E. 연관 규칙 학습 (Association Rule Learning)

**정답:** B
**해설:** 회귀는 입력 변수와 연속적인 출력 변수 간의 관계를 모델링하여, 새로운 입력에 대한 연속적인 값을 예측하는 작업입니다.
- A: 데이터를 미리 정의된 범주(클래스)로 나누는 작업입니다.
- C, D: 대표적인 비지도 학습의 종류입니다.

---

## 문제 3 (난이도: 보통)
이진 분류를 위해 선형 회귀 모델의 출력에 시그모이드(Sigmoid) 함수를 적용하여 결과를 0과 1 사이의 확률 값으로 변환하는 모델은 무엇인가?

### 보기
A. 서포트 벡터 머신 (SVM)
B. K-최근접 이웃 (K-NN)
C. 결정 트리 (Decision Tree)
D. 로지스틱 회귀 (Logistic Regression)
E. 주성분 분석 (PCA)

**정답:** D
**해설:** 로지스틱 회귀는 선형 회귀의 예측값을 시그모이드 함수에 통과시켜 0에서 1 사이의 확률로 변환하고, 이 확률을 기준으로 데이터를 두 개의 클래스 중 하나로 분류하는 모델입니다.

---

## 문제 4 (난이도: 보통)
모델의 손실 함수(Loss Function)를 최소화하는 파라미터를 찾기 위해, 기울기의 반대 방향으로 파라미터를 점진적으로 업데이트하는 최적화 알고리즘은 무엇인가?

### 보기
A. 정규 방정식 (Normal Equation)
B. 최소 제곱법 (Least Squares Method)
C. 경사 하강법 (Gradient Descent)
D. K-평균 군집화 (K-Means Clustering)
E. 주성분 분석 (PCA)

**정답:** C
**해설:** 경사 하강법은 손실 함수의 기울기가 가장 가파른 방향의 반대쪽으로 파라미터를 조금씩 이동시켜, 점진적으로 손실이 가장 낮은 지점(최적점)을 찾아가는 반복적인 최적화 방법입니다. 딥러닝에서 가장 기본이 되는 최적화 원리입니다.

---

## 문제 5 (난이도: 쉬움)
데이터 탐색(EDA) 과정에서, 두 변수 간의 관계가 얼마나 강한지를 -1과 1 사이의 값으로 나타내는 지표는 무엇인가?

### 보기
A. 평균 제곱 오차 (MSE)
B. 상관계수 (Correlation Coefficient)
C. 사분위수 범위 (IQR)
D. 정확도 (Accuracy)
E. P-값 (P-value)

**정답:** B
**해설:** 상관계수는 두 변수가 함께 변화하는 정도를 나타내는 통계적 척도입니다. 1에 가까울수록 강한 양의 상관관계, -1에 가까울수록 강한 음의 상관관계를 의미하며, 0에 가까우면 선형적인 관계가 없음을 의미합니다.

---

## 문제 6 (난이도: 보통)
모델이 훈련 데이터에만 과도하게 최적화되어, 새로운 테스트 데이터에 대해서는 성능이 현저히 낮아지는 현상은 무엇인가?

### 보기
A. 과소적합 (Underfitting)
B. 과적합 (Overfitting)
C. 데이터 유출 (Data Leakage)
D. 일반화 (Generalization)
E. 경사 소실 (Gradient Vanishing)

**정답:** B
**해설:** 과적합은 모델이 훈련 데이터의 노이즈나 특정 패턴까지 모두 외워버려서, 새로운 데이터에 대한 일반화 성능이 떨어지는 현상을 말합니다. 모델의 복잡도가 너무 높거나 훈련 데이터가 부족할 때 주로 발생합니다.

---

## 문제 7 (난이도: 보통)
데이터의 특성(feature)마다 값의 범위(scale)가 다를 때, 모든 데이터 포인트의 평균을 0, 표준편차를 1로 변환하는 전처리 기법은 무엇인가?

### 보기
A. 정규화 (Normalization)
B. 표준화 (Standardization)
C. 원-핫 인코딩 (One-Hot Encoding)
D. 차원 축소 (Dimensionality Reduction)
E. 이상치 제거 (Outlier Removal)

**정답:** B
**해설:** 표준화는 각 특성의 스케일을 통일하여 모든 특성이 모델 학습에 공평하게 영향을 미치도록 하는 기법입니다. 특히 거리 기반 알고리즘이나 경사 하강법을 사용하는 모델에서 성능 향상에 중요한 역할을 합니다.

---

## 문제 8 (난이도: 보통)
분류 모델의 성능 평가 지표 중, 실제 Positive인 데이터 중에서 모델이 Positive라고 올바르게 예측한 비율을 무엇이라고 하는가?

### 보기
A. 정확도 (Accuracy)
B. 정밀도 (Precision)
C. F1-점수 (F1-Score)
D. 재현율 (Recall)
E. 특이도 (Specificity)

**정답:** D
**해설:** 재현율(Recall) 또는 민감도(Sensitivity)는 실제 참(Positive)인 것들 중 모델이 얼마나 많이 참으로 예측했는지를 나타냅니다. 즉, 모델이 '놓치지 않고 얼마나 잘 찾아내는가'를 측정하는 지표로, 암 진단과 같이 실제 Positive를 놓치면 안 되는 경우에 특히 중요합니다.

---

## 문제 9 (난이도: 어려움)
정밀도(Precision)와 재현율(Recall)은 일반적으로 한쪽이 높아지면 다른 쪽이 낮아지는 트레이드오프(Trade-off) 관계에 있다. 다음 중 이 관계에 대한 설명으로 가장 옳지 않은 것은?

### 보기
A. 모델의 예측 임계값(threshold)을 높이면, 모델이 Positive로 예측하는 데 더 신중해져 정밀도는 높아지고 재현율은 낮아지는 경향이 있다.
B. 모델의 예측 임계값을 낮추면, 모델이 Positive로 더 관대하게 예측하여 재현율은 높아지고 정밀도는 낮아지는 경향이 있다.
C. F1-점수는 정밀도와 재현율의 조화 평균으로, 두 지표가 모두 높을 때 함께 높아지므로 트레이드오프 관계를 종합적으로 고려하는 지표이다.
D. 스팸 메일 분류기에서는 재현율이 정밀도보다 항상 더 중요하다.
E. 암 진단 모델에서는 실제 환자를 놓치면 안 되므로(FN 최소화), 정밀도가 다소 낮아지더라도 재현율을 높이는 것이 더 중요하다.

**정답:** D
**해설:** 스팸 메일 분류기에서는 정상 메일을 스팸으로 잘못 분류하는 경우(FP)가 사용자에게 더 큰 불편을 초래할 수 있습니다. 따라서 모델이 스팸이라고 예측한 것이 정말 스팸일 확률, 즉 정밀도(Precision)를 더 중요하게 고려하는 경우가 많습니다. 어떤 지표가 더 중요한지는 해결하려는 문제의 특성에 따라 달라집니다.

---

## 문제 10 (난이도: 보통)
데이터셋이 제한적일 때 모델의 일반화 성능을 더 신뢰성 있게 평가하기 위해, 훈련 데이터를 여러 개(K개)의 폴드(fold)로 나누어 각 폴드가 한 번씩 검증 데이터 역할을 하도록 반복적으로 평가하는 방법은 무엇인가?

### 보기
A. 홀드아웃 검증 (Hold-out Validation)
B. 부트스트래핑 (Bootstrapping)
C. K-폴드 교차 검증 (K-Fold Cross-Validation)
D. 그리드 서치 (Grid Search)
E. 랜덤 서치 (Random Search)

**정답:** C
**해설:** K-폴드 교차 검증은 데이터를 어떻게 나누느냐에 따라 모델의 성능 평가가 달라지는 것을 방지하기 위한 기법입니다. 전체 훈련 데이터를 K개의 부분집합으로 나누고, K번의 학습과 평가를 반복하며 각 부분집합이 한 번씩 검증 세트로 사용됩니다. 이후 K개의 성능 점수를 평균 내어 최종 성능을 측정합니다.

---

## 문제 11 (난이도: 보통)
딥러닝 모델의 학습 과정에서, 손실(loss)을 기반으로 출력층에서부터 입력층 방향으로 각 가중치에 대한 기울기를 효율적으로 계산하는 알고리즘은 무엇인가?

### 보기
A. 순전파 (Forward Propagation)
B. 역전파 (Backpropagation)
C. 경사 하강법 (Gradient Descent)
D. 드롭아웃 (Dropout)
E. 활성화 함수 (Activation Function)

**정답:** B
**해설:** 역전파 알고리즘은 미분의 연쇄 법칙(Chain Rule)을 활용하여, 출력층의 손실로부터 시작해 거꾸로 한 단계씩 미분값을 계산하며 전체 파라미터에 대한 기울기를 효율적으로 구하는 방법입니다. 이 기울기 값을 경사 하강법에 사용하여 가중치를 업데이트합니다.

---

## 문제 12 (난이도: 어려움)
경사 하강법 기반의 옵티마이저(Optimizer) 중, 각 파라미터마다 적응적인(adaptive) 학습률을 적용하면서도, 과거의 기울기 정보를 관성(momentum)처럼 활용하여 지역 최적점(local minimum)을 더 잘 탈출하고 수렴 속도를 높이는, 현재 가장 널리 사용되는 옵티마이저는 무엇인가?

### 보기
A. 확률적 경사 하강법 (SGD)
B. 모멘텀 (Momentum)
C. RMSprop
D. Adam
E. Adagrad

**정답:** D
**해설:** Adam(Adaptive Moment Estimation) 옵티마이저는 RMSprop과 Momentum 방식을 결합한 것과 같습니다. 각 파라미터별로 학습률을 적응적으로 조절하는 능력(RMSprop의 특징)과, 이전 기울기 방향을 유지하려는 관성(Momentum의 특징)을 모두 가지고 있어, 대부분의 딥러닝 문제에서 빠르고 안정적인 성능을 보여 널리 사용됩니다.

---

## 문제 13 (난이도: 보통)
과적합을 방지하기 위한 규제(Regularization) 기법 중, 학습 과정에서 신경망의 일부 뉴런을 무작위로 비활성화하여 모델이 특정 뉴런에 과도하게 의존하는 것을 막는 기법은 무엇인가?

### 보기
A. L1 규제 (L1 Regularization)
B. L2 규제 (L2 Regularization)
C. 조기 종료 (Early Stopping)
D. 드롭아웃 (Dropout)
E. 데이터 증강 (Data Augmentation)

**정답:** D
**해설:** 드롭아웃은 각 학습 스텝마다 지정된 비율만큼의 뉴런을 무작위로 '끄는' 기법입니다. 이는 마치 매번 다른 구조의 작은 모델 여러 개를 학습시켜 앙상블하는 것과 유사한 효과를 내어, 모델의 일반화 성능을 높이고 과적합을 효과적으로 방지합니다.

---

## 문제 14 (난이도: 보통)
자연어 처리에서 단어의 의미적 유사성을 벡터 공간에서의 거리로 표현할 수 있는 밀집 벡터(dense vector) 표현법은 무엇인가?

### 보기
A. 원-핫 인코딩 (One-Hot Encoding)
B. TF-IDF
C. N-gram
D. Word2Vec
E. Bag-of-Words (BoW)

**정답:** D
**해설:** Word2Vec은 단어의 의미를 다차원 공간의 밀집 벡터(임베딩)로 표현하는 기법입니다. 주변 단어와의 관계를 기반으로 학습하여, 의미적으로 유사한 단어들이 벡터 공간에서 가깝게 위치하게 됩니다. 이로 인해 단어 간의 의미론적, 문법적 관계를 벡터 연산으로 표현할 수 있습니다.

---

## 문제 15 (난이도: 보통)
순환 신경망(RNN)이 긴 시퀀스를 처리할 때, 정보가 뒤로 전달될수록 기울기가 점차 사라져 장기 의존성(Long-term dependency) 학습이 어려워지는 고질적인 문제는 무엇인가?

### 보기
A. 차원의 저주 (Curse of Dimensionality)
B. 기울기 소실 문제 (Vanishing Gradient Problem)
C. 과적합 문제 (Overfitting Problem)
D. 병목 현상 (Bottleneck Problem)
E. 계산 복잡성 증가 (Increased Computational Complexity)

**정답:** B
**해설:** RNN은 역전파 과정에서 동일한 가중치 행렬을 시퀀스의 길이만큼 반복적으로 곱하게 됩니다. 이때 행렬의 특정 값들이 1보다 작으면, 계속된 곱셈으로 인해 기울기가 0에 가까워져 사라지게 됩니다. 이로 인해 모델이 시퀀스의 앞부분에 있는 오래된 정보를 학습하지 못하는 문제가 발생합니다.

---

## 문제 16 (난이도: 어려움)
LSTM(Long Short-Term Memory)이 RNN의 기울기 소실 문제를 해결하는 핵심 원리로 가장 적절한 것은 무엇인가?

### 보기
A. 역전파 과정에서 가중치를 곱하는 대신 더하는 연산을 사용한다.
B. 셀 상태(Cell State)라는 별도의 정보 경로를 두고, 게이트(Gate)들을 통해 정보의 흐름을 제어하여 기울기가 소실되지 않고 오래 유지되도록 한다.
C. 모든 은닉 상태를 다음 레이어로 직접 연결하여 정보 손실을 막는다.
D. 활성화 함수로 ReLU를 사용하여 음수 기울기를 0으로 만든다.
E. 어텐션 메커니즘을 도입하여 중요한 정보에만 집중한다.

**정답:** B
**해설:** LSTM의 핵심은 셀 상태(Cell State)입니다. 이 셀 상태는 기존 RNN의 은닉 상태와는 별도로, 컨베이어 벨트처럼 시퀀스의 처음부터 끝까지 거의 변하지 않고 정보를 전달할 수 있습니다. 망각, 입력, 출력 게이트는 이 셀 상태에 정보를 얼마나 더하고 뺄지를 제어하는 역할을 하며, 역전파 시 기울기가 이 셀 상태를 통해 거의 그대로 전달될 수 있어 기울기 소실 문제가 크게 완화됩니다. A는 잔차 연결(ResNet)의 원리와 더 가깝습니다.

---

## 문제 17 (난이도: 보통)
Seq2Seq 모델에서 입력 시퀀스의 모든 정보를 고정된 크기의 벡터 하나(컨텍스트 벡터)에 압축할 때 발생하는 정보 손실 문제(Bottleneck Problem)를 해결하기 위해 등장한 메커니즘은 무엇인가?

### 보기
A. 어텐션 (Attention)
B. 드롭아웃 (Dropout)
C. 배치 정규화 (Batch Normalization)
D. 잔여 연결 (Residual Connection)
E. 교차 검증 (Cross-Validation)

**정답:** A
**해설:** 어텐션 메커니즘은 디코더가 각 타임스텝에서 단어를 생성할 때마다, 인코더의 모든 은닉 상태를 다시 참조하여 현재 생성할 단어와 가장 관련이 깊은 입력 단어에 더 높은 가중치(attention)를 부여합니다. 이를 통해 고정된 컨텍스트 벡터에만 의존하지 않고, 필요할 때마다 입력 시퀀스의 중요한 정보를 직접 가져와 사용하여 성능을 크게 향상시켰습니다.

---

## 문제 18 (난이도: 어려움)
트랜스포머(Transformer)의 셀프 어텐션(Self-Attention) 메커니즘에 대한 설명으로 옳지 않은 것은?

### 보기
A. 각 단어로부터 Query, Key, Value라는 세 가지 벡터를 생성하여 단어 간의 관계를 계산한다.
B. 재귀(Recurrence) 구조가 없어 시퀀스 내 모든 단어에 대한 계산을 병렬적으로 처리할 수 있다.
C. 셀프 어텐션 자체만으로는 단어의 순서 정보를 처리할 수 없어, 위치 인코딩(Positional Encoding)을 별도로 추가해야 한다.
D. 어텐션 가중치를 계산하는 과정은 본질적으로 RNN 레이어를 여러 개 쌓은 것과 동일하다.
E. 여러 개의 어텐션 헤드를 사용하는 멀티-헤드 어텐션(Multi-Head Attention)을 통해 다양한 관점의 관계를 동시에 학습할 수 있다.

**정답:** D
**해설:** 셀프 어텐션은 RNN 구조를 완전히 배제하고 행렬 곱 연산에 기반하여 단어 간의 관계를 직접 계산하는 메커니즘입니다. 이는 순차적으로 정보를 처리하는 RNN과는 근본적으로 다른 방식이며, 트랜스포머가 병렬 처리와 장거리 의존성 학습에 강점을 갖게 된 핵심적인 이유입니다.

---

## 문제 19 (난이도: 보통)
CNN(합성곱 신경망)에서 이미지의 지역적 특징(local feature)을 추출하기 위해 사용되는 핵심적인 연산은 무엇인가?

### 보기
A. 풀링 (Pooling)
B. 합성곱 (Convolution)
C. 완전 연결 (Fully Connection)
D. 드롭아웃 (Dropout)
E. 역전파 (Backpropagation)

**정답:** B
**해설:** 합성곱 연산은 필터(커널)를 이미지 위에서 이동시키며 지역적인 패턴(모서리, 질감 등)을 감지하고 특징 맵(Feature Map)을 생성하는 과정입니다. 이는 이미지 전체를 한 번에 처리하는 대신 지역 정보를 효과적으로 추출하는 CNN의 핵심 원리입니다.

---

## 문제 20 (난이도: 보통)
CNN에서 합성곱 층을 통과한 특징 맵의 크기를 줄여 계산량을 감소시키고, 약간의 위치 변화에도 모델이 강건하게 반응하도록 만드는 연산은 무엇인가?

### 보기
A. 스트라이드 (Stride)
B. 패딩 (Padding)
C. 풀링 (Pooling)
D. 정규화 (Normalization)
E. 활성화 함수 (Activation Function)

**정답:** C
**해설:** 풀링(주로 Max Pooling)은 특징 맵의 특정 영역에서 가장 중요한 특징(최댓값)만을 남기고 나머지는 버리는 다운샘플링(down-sampling) 과정입니다. 이를 통해 데이터의 크기를 줄여 연산을 효율화하고, 사소한 변화에 덜 민감한, 더 강건한 특징을 추출할 수 있습니다.

---

## 문제 21 (난이도: 어려움)
네트워크가 매우 깊어질 때 오히려 훈련 에러가 증가하는 성능 저하 문제(Degradation Problem)를 해결하기 위해, 입력값을 출력값에 그대로 더해주는 '스킵 연결(Skip Connection)' 구조를 도입한 혁신적인 CNN 아키텍처는 무엇인가?

### 보기
A. VGGNet
B. AlexNet
C. GoogleNet
D. ResNet
E. MobileNet

**정답:** D
**해설:** ResNet(Residual Network)은 잔차 연결(Residual Connection) 또는 스킵 연결이라 불리는 구조를 통해, 층이 깊어지더라도 학습이 용이하도록 만들었습니다. 이는 기울기 소실 문제를 완화하고, 이전까지 불가능했던 수백, 수천 층의 매우 깊은 신경망을 성공적으로 학습시키는 길을 열었습니다. 문제에서 언급된 성능 저하 문제는 과소적합의 한 형태로 볼 수 있습니다.

---

## 문제 22 (난이도: 보통)
자연어 처리에서 성공한 트랜스포머 구조를 컴퓨터 비전 분야에 적용하여, 이미지를 여러 개의 패치(patch)로 나누고 이를 단어 시퀀스처럼 처리하는 새로운 패러다임을 제시한 모델은 무엇인가?

### 보기
A. ResNet
B. EfficientNet
C. ViT (Vision Transformer)
D. FCN (Fully Convolutional Network)
E. YOLO

**정답:** C
**해설:** ViT는 CNN의 합성곱 연산 대신, 이미지를 16x16 같은 고정된 크기의 패치로 분할하고 각 패치를 트랜스포머의 입력 토큰으로 사용합니다. 이를 통해 셀프 어텐션 메커니즘으로 이미지 전체의 전역적인 관계를 학습하며, 대규모 데이터셋에서 CNN을 능가하는 성능을 보여주었습니다.

---

## 문제 23 (난이도: 보통)
거대 언어 모델(LLM)의 규모가 커질수록 성능이 예측 가능하게 향상된다는 법칙과, 특정 규모를 넘어서면서 질적으로 다른 새로운 능력이 갑자기 나타나는 현상을 각각 올바르게 짝지은 것은?

### 보기
A. 창발성, 규모의 법칙
B. 규모의 법칙, 창발성
C. 무어의 법칙, 창발성
D. 규모의 법칙, 과적합
E. 인-컨텍스트 학습, 규모의 법칙

**정답:** B
**해설:** 규모의 법칙(Scaling Law)은 모델 크기, 데이터셋 크기, 학습 계산량이라는 세 가지 요소가 커질수록 모델의 손실(loss)이 예측 가능한 방식으로 감소한다는 것을 의미합니다. 창발성(Emergent Property)은 이러한 양적 증가가 계속되다 보면, 특정 규모에서 예측하지 못했던 질적인 변화, 즉 새로운 능력(예: 인-컨텍스트 학습, 복잡한 추론)이 갑자기 발현되는 현상을 말합니다.

---

## 문제 24 (난이도: 보통)
LLM의 한계인 '환각(Hallucination)'과 '지식 단절(Knowledge Cut-off)' 문제를 해결하기 위해, 외부 데이터베이스에서 관련 문서를 검색(Retrieve)하여 LLM의 입력 프롬프트에 함께 제공하는 기법은 무엇인가?

### 보기
A. 파인튜닝 (Fine-tuning)
B. 프롬프트 엔지니어링 (Prompt Engineering)
C. 검색 증강 생성 (RAG, Retrieval-Augmented Generation)
D. 지식 증류 (Knowledge Distillation)
E. 인-컨텍스트 학습 (In-context Learning)

**정답:** C
**해설:** RAG는 LLM의 내부 지식에만 의존하지 않고, 질문과 관련된 정보를 외부 지식 소스에서 실시간으로 검색하여 이를 LLM의 컨텍스트에 추가하여 더 정확하고 사실에 기반한 답변을 생성하도록 유도합니다.

---

## 문제 25 (난이도: 어려움)
ChatGPT의 핵심 기술인 RLHF(인간 피드백 기반 강화학습)에 대한 설명으로 가장 옳지 않은 것은?

### 보기
A. 인간의 주관적인 선호를 학습하기 위해, 여러 답변에 대한 인간의 순위 평가 데이터를 사용한다.
B. 학습된 보상 모델(Reward Model)은 어떤 답변이 더 좋은 답변인지를 판단하는 판별자 역할을 한다.
C. 최종적으로 LLM은 강화학습을 통해 보상 모델로부터 높은 점수를 받는 방향으로 파인튜닝된다.
D. RLHF는 모델의 유용성과 안전성을 크게 향상시켰지만, 보상 모델의 허점을 이용하는 '보상 해킹(Reward Hacking)' 문제가 발생할 수 있다.
E. RLHF는 추가적인 데이터나 학습 없이 사전 학습된 모델을 그대로 사용하여 인간의 피드백에 맞춰 응답을 생성하는 기술이다.

**정답:** E
**해설:** RLHF는 사전 학습된 모델을 기반으로 하지만, (1) 지시 학습, (2) 보상 모델 학습, (3) 강화학습을 통한 파인튜닝이라는 여러 단계의 복잡하고 비용이 많이 드는 '추가 학습' 과정을 거칩니다. 사전 학습된 모델을 그대로 사용하는 것이 아닙니다.

---

## 문제 26 (난이도: 보통)
LLM이 텍스트를 생성할 때, 확률적으로 가장 가능성 있는 상위 K개의 토큰들 중에서만 다음 토큰을 샘플링하는 디코딩 전략은 무엇인가?

### 보기
A. Greedy Decoding
B. Beam Search
C. Top-K Sampling
D. Top-P (Nucleus) Sampling
E. Temperature Sampling

**정답:** C
**해설:** Top-K 샘플링은 확률이 매우 낮은 이상한 토큰들이 선택되는 것을 방지하면서도, Greedy 방식처럼 단 하나의 선택지에 얽매이지 않도록 하는 방법입니다. 확률 분포에서 가장 확률이 높은 K개의 토큰으로 후보군을 제한하고, 그 안에서 확률에 비례하여 다음 토큰을 랜덤하게 선택하여 적절한 수준의 일관성과 다양성을 모두 확보합니다.

---

## 문제 27 (난이도: 어려움)
LLM의 텍스트 생성 디코딩 전략 중, Top-K 샘플링과 Top-P(Nucleus) 샘플링의 가장 핵심적인 차이점은 무엇인가?

### 보기
A. Top-K는 확률 임계값을, Top-P는 후보 토큰의 개수를 고정한다.
B. Top-P는 확률 분포의 모양에 따라 후보군의 크기(K)를 동적으로 조절하지만, Top-K는 항상 고정된 수의 후보군을 고려한다.
C. Top-K는 항상 Top-P보다 더 다양하고 창의적인 문장을 생성한다.
D. Top-P는 Beam Search와 결합할 수 없지만, Top-K는 결합할 수 있다.
E. Top-K는 계산 비용이 매우 비싸지만, Top-P는 매우 저렴하다.

**정답:** B
**해설:** Top-K는 확률 분포가 얼마나 뾰족하든 평평하든 상관없이 무조건 상위 K개의 토큰만 후보로 삼습니다. 반면, Top-P는 누적 확률이 P가 될 때까지의 토큰들만 후보로 삼기 때문에, 모델이 특정 토큰을 강하게 확신할 때는 후보군이 적어지고(예: 1~2개), 여러 토큰을 비슷하게 고려할 때는 후보군이 많아지는 등 확률 분포에 따라 유연하게 후보군의 크기를 조절하는 장점이 있습니다.

---

## 문제 28 (난이도: 보통)
RAG 시스템에서, 키워드의 등장 빈도에 기반한 전통적인 검색(Sparse Retrieval)과 문장의 의미적 유사도에 기반한 벡터 검색(Dense Retrieval)을 함께 사용하여 검색 성능을 극대화하는 접근 방식을 무엇이라고 하는가?

### 보기
A. 교차 인코더 검색 (Cross-encoder Search)
B. 이중 인코더 검색 (Bi-encoder Search)
C. 하이브리드 검색 (Hybrid Search)
D. 계층적 검색 (Hierarchical Search)
E. 그래프 기반 검색 (Graph-based Search)

**정답:** C
**해설:** 하이브리드 검색은 Sparse Retriever(예: TF-IDF, BM25)와 Dense Retriever(벡터 검색)의 장점을 결합한 방식입니다. Sparse 검색은 키워드 정확성이 높고, Dense 검색은 의미적 유연성이 높기 때문에, 두 방식의 점수를 결합하여 순위를 매기면 각각의 단점을 보완하고 전반적인 검색 관련성을 크게 향상시킬 수 있습니다.

---

## 문제 29 (난이도: 어려움)
Dense Retriever에서 사용되는 Bi-encoder와 Cross-encoder 아키텍처에 대한 설명으로 가장 옳지 않은 것은?

### 보기
A. Bi-encoder는 질문과 문서를 각각 독립적으로 임베딩하므로, 대규모 문서에 대한 빠른 검색이 가능하다.
B. Cross-encoder는 질문과 문서를 쌍으로 묶어 한 번에 처리하므로, 둘 사이의 미묘한 상호작용을 잘 포착하여 정확도가 높다.
C. 효율적인 RAG 시스템은 1단계에서 Cross-encoder로 후보군을 빠르게 추리고, 2단계에서 Bi-encoder로 정확한 순위를 매긴다.
D. Bi-encoder는 사전 계산(pre-computation)이 가능하지만, Cross-encoder는 검색 시점에 모든 후보 쌍에 대해 연산을 수행해야 한다.
E. Cross-encoder는 높은 계산 비용 때문에 실시간 검색 시스템의 메인 검색기로는 부적합할 수 있다.

**정답:** C
**해설:** 효율적인 2단계 검색 시스템에서는 순서가 반대로 적용됩니다. 1단계에서는 속도가 빠른 Bi-encoder나 Sparse Retriever를 사용하여 수백만 개의 문서 중에서 관련성이 높은 상위 수십~수백 개의 후보 문서를 빠르게 걸러냅니다. 그 다음 2단계에서, 이 소수의 후보군에 대해서만 계산 비용이 비싸지만 정확도가 높은 Cross-encoder를 적용하여 최종 순위를 정교하게 매기는(re-ranking) 방식을 사용합니다.

---

## 문제 30 (난이도: 보통)
수십억 개의 파라미터를 가진 LLM 전체를 파인튜닝하는 대신, 기존 모델의 가중치는 고정한 채 소수의 파라미터만 추가하여 학습함으로써 계산 효율을 크게 높이는 PEFT(Parameter-Efficient Fine-Tuning)의 대표적인 기법은 무엇인가?

### 보기
A. 양자화 (Quantization)
B. 가지치기 (Pruning)
C. LoRA (Low-Rank Adaptation)
D. 지식 증류 (Knowledge Distillation)
E. 전이 학습 (Transfer Learning)

**정답:** C
**해설:** LoRA는 PEFT 기법 중 가장 성공적이고 널리 사용되는 방법 중 하나입니다. 사전 학습된 가중치의 변화량이 저-랭크(low-rank) 행렬로 근사될 수 있다는 아이디어에 기반하여, 원래의 가중치는 그대로 두고 저-랭크 행렬에 해당하는 매우 적은 수의 파라미터만 학습하여 높은 효율과 준수한 성능을 동시에 달성합니다.

---

## 문제 31 (난이도: 어려움)
LoRA 설정 시 `r` (rank)과 `lora_alpha`의 관계에 대한 설명으로 가장 적절한 것은?

### 보기
A. `r`은 학습 파라미터 수를, `lora_alpha`는 학습률을 결정한다.
B. `lora_alpha`는 `r`과 독립적이며, 항상 1로 설정해야 한다.
C. `lora_alpha`는 LoRA 어댑터의 출력에 곱해지는 스케일링 인자로, 보통 `r` 값의 두 배로 설정하여 `r`의 변화에 따른 스케일 변화를 보정하는 역할을 한다.
D. `r` 값을 키울수록 항상 모델의 성능이 좋아지므로, `lora_alpha`는 고정하고 `r`만 최대한 키우는 것이 좋다.
E. `lora_alpha`는 LoRA를 적용할 레이어의 개수를 의미한다.

**정답:** C
**해설:** LoRA의 출력은 `(BA)x * (lora_alpha / r)`로 스케일링됩니다. `r` 값을 바꾸면 학습 가능한 파라미터의 수가 바뀌면서 어댑터 출력의 크기(magnitude)도 함께 변하게 됩니다. 이때 `lora_alpha`를 `r`에 비례하는 값(예: 2*r)으로 설정하면, `r`을 바꿔도 최종적인 스케일링 효과는 일정하게 유지되어 하이퍼파라미터 튜닝이 더 용이해집니다. 즉, `lora_alpha`는 학습률과 같은 역할을 하는 스케일링 팩터로 볼 수 있습니다.

---

## 문제 32 (난이도: 보통)
LoRA와 4비트 양자화(Quantization)를 결합하여, GPU 메모리 사용량을 획기적으로 줄이면서도 준수한 성능을 유지하여 일반 소비자용 GPU에서도 거대 언어 모델의 파인튜닝을 가능하게 만든 기술은 무엇인가?

### 보기
A. AdaLoRA
B. LongLoRA
C. QLoRA
D. SparseLoRA
E. Unsloth

**정답:** C
**해설:** QLoRA는 사전 학습된 모델의 전체 가중치를 4비트로 양자화하여 메모리에 로드하고, 이 양자화된 모델 위에 LoRA 어댑터를 추가하여 파인튜닝을 진행하는 방식입니다. 이를 통해 16비트 모델을 직접 로드할 때보다 메모리 요구량을 극적으로 줄여, 제한된 하드웨어 환경에서도 LLM 파인튜닝의 접근성을 크게 높였습니다.

---

## 문제 33 (난이도: 보통)
모델의 가중치나 활성화 값을 더 낮은 정밀도의 데이터 타입(예: FP32 → FP16 또는 INT8)으로 변환하여 모델의 크기와 메모리 사용량을 줄이는 모델 압축 기술은 무엇인가?

### 보기
A. 가지치기 (Pruning)
B. 양자화 (Quantization)
C. 지식 증류 (Knowledge Distillation)
D. 드롭아웃 (Dropout)
E. 정규화 (Normalization)

**정답:** B
**해설:** 양자화는 모델의 파라미터를 표현하는 데 필요한 비트 수를 줄이는 기술의 총칭입니다. 예를 들어, 32비트 부동소수점으로 저장된 가중치를 8비트 정수로 변환하면 모델 크기는 1/4로 줄어들고, 정수 연산에 최적화된 하드웨어에서는 추론 속도가 빨라지는 효과를 얻을 수 있습니다.

---

## 문제 34 (난이도: 어려움)
양자화(Quantization) 기법 중, 학습 과정 중에 양자화를 시뮬레이션하여 정확도 손실을 최소화하는 QAT(Quantization-Aware Training)에 대한 설명으로 가장 옳지 않은 것은?

### 보기
A. 학습 시 순전파 과정에서는 가중치를 양자화한 것처럼 시뮬레이션하여 오차를 반영한다.
B. 역전파 시에는 양자화된 가중치가 아닌 실제 고정밀 가중치를 업데이트한다.
C. PTQ(Post-Training Quantization)에 비해 더 높은 정확도를 유지할 수 있다.
D. 별도의 학습 데이터나 파인튜닝 과정 없이, 이미 학습된 모델에 바로 적용할 수 있어 매우 간편하다.
E. 양자화로 인한 성능 저하를 모델이 학습 과정에서 스스로 보정하도록 유도하는 방식이다.

**정답:** D
**해설:** QAT는 이름 그대로 '양자화를 인지하며 학습'하는 방식이므로, 반드시 학습 데이터와 추가적인 학습(파인튜닝) 과정이 필요합니다. 이는 QAT의 가장 큰 단점이자, 적용이 간편한 PTQ와의 핵심적인 차이점입니다.

---

## 문제 35 (난이도: 보통)
모델의 성능에 거의 영향을 주지 않는 작은 값의 가중치 연결을 영구적으로 제거하여, 모델의 파라미터 수를 줄이고 희소(sparse) 모델로 만드는 압축 기술은 무엇인가?

### 보기
A. 양자화 (Quantization)
B. 드롭아웃 (Dropout)
C. 가중치 감쇠 (Weight Decay)
D. 가지치기 (Pruning)
E. 지식 증류 (Knowledge Distillation)

**정답:** D
**해설:** 가지치기(Pruning)는 모델의 복잡도를 줄이는 효과적인 방법 중 하나입니다. 중요도가 낮은 가중치를 제거함으로써 모델의 크기를 줄이고, 경우에 따라서는 추론 속도를 향상시킬 수 있습니다. 제거하는 단위에 따라 비정형(Unstructured) 가지치기와 정형(Structured) 가지치기로 나뉩니다.

---

## 문제 36 (난이도: 어려움)
가지치기(Pruning) 기법 중, 정형(Structured) 가지치기와 비정형(Unstructured) 가지치기에 대한 설명으로 가장 적절한 것은?

### 보기
A. 정형 가지치기는 개별 가중치를 제거하여 압축률이 높지만, 비정형 가지치기는 채널이나 필터 단위로 제거하여 하드웨어 가속에 유리하다.
B. 비정형 가지치기는 개별 가중치를 제거하여 압축률이 높지만, 정형 가지치기는 채널이나 필터 단위로 제거하여 하드웨어 가속에 유리하다.
C. 정형 가지치기는 항상 비정형 가지치기보다 높은 정확도를 보인다.
D. 비정형 가지치기는 모델의 구조를 변경하지 않으므로 추가적인 컴파일러 지원이 필요 없다.
E. 두 방식 모두 추론 시 연산량(FLOPs)을 동일한 수준으로 감소시킨다.

**정답:** B
**해설:** 두 방식의 핵심적인 트레이드오프 관계입니다. 비정형 가지치기는 자유롭게 개별 가중치를 제거하므로 모델 정확도 손실을 최소화하면서 높은 압축률을 달성할 수 있지만, 결과적인 행렬이 불규칙한 패턴을 가져 일반 하드웨어에서 가속하기 어렵습니다. 반면, 정형 가지치기는 필터나 채널 등 규칙적인 블록 단위로 제거하므로 압축률은 낮을 수 있지만, 모델 구조가 유지되어 하드웨어 가속에 매우 유리합니다.

---

## 문제 37 (난이도: 보통)
크고 복잡한 '교사 모델'이 가진 지식(예: 클래스 간의 미묘한 확률 분포)을 작고 효율적인 '학생 모델'이 학습하도록 하여, 모델의 크기는 줄이면서도 성능은 최대한 보존하려는 모델 압축 기법은 무엇인가?

### 보기
A. 전이 학습 (Transfer Learning)
B. 지식 증류 (Knowledge Distillation)
C. 메타 학습 (Meta-Learning)
D. 자기 지도 학습 (Self-supervised Learning)
E. 앙상블 학습 (Ensemble Learning)

**정답:** B
**해설:** 지식 증류는 교사 모델의 'soft labels'(예: [0.1, 0.2, 0.7])를 학생 모델의 학습 타겟으로 사용하여, 정답인 'hard label'([0, 0, 1])만 사용하는 것보다 더 풍부한 정보를 전달하는 방식입니다. 이를 통해 학생 모델은 더 적은 파라미터로도 교사 모델과 유사한 일반화 성능을 달성할 수 있습니다.

---

## 문제 38 (난이도: 쉬움)
비지도 학습의 한 종류로, 데이터 포인트들을 유사성에 따라 여러 개의 그룹(cluster)으로 묶는 작업을 무엇이라고 하는가?

### 보기
A. 회귀 (Regression)
B. 분류 (Classification)
C. 군집화 (Clustering)
D. 차원 축소 (Dimensionality Reduction)
E. 강화 학습 (Reinforcement Learning)

**정답:** C
**해설:** 군집화는 정답 레이블이 없는 데이터 내에서 숨겨진 구조를 발견하는 대표적인 비지도 학습 방법입니다. 데이터의 특성을 기반으로 서로 유사한 데이터들을 하나의 그룹으로 묶어, 데이터의 분포나 특성을 파악하는 데 사용됩니다.

---

## 문제 39 (난이도: 보통)
데이터의 특성(feature)이 너무 많을 때 발생하는 '차원의 저주' 문제를 해결하고, 데이터의 핵심적인 구조를 유지하면서 특성의 개수를 줄이는 비지도 학습 기법은 무엇인가?

### 보기
A. K-평균 군집화 (K-Means Clustering)
B. 주성분 분석 (PCA, Principal Component Analysis)
C. 선형 회귀 (Linear Regression)
D. 로지스틱 회귀 (Logistic Regression)
E. 서포트 벡터 머신 (SVM)

**정답:** B
**해설:** PCA는 데이터의 분산이 가장 큰 방향을 새로운 주성분(축)으로 설정하여, 원래의 특성 공간을 더 낮은 차원의 새로운 특성 공간으로 변환하는 대표적인 차원 축소 기법입니다. 이를 통해 데이터의 중요한 정보는 최대한 유지하면서 계산 효율성을 높이고 시각화를 용이하게 합니다.

---

## 문제 40 (난이도: 보통)
데이터 시각화에서, 데이터의 분포를 사분위수(quartile)를 이용하여 표현하며, 이상치(outlier)를 탐지하는 데 매우 효과적인 그래프는 무엇인가?

### 보기
A. 히스토그램 (Histogram)
B. 산점도 (Scatter Plot)
C. 선 그래프 (Line Graph)
D. 박스 플롯 (Box Plot)
E. 히트맵 (Heatmap)

**정답:** D
**해설:** 박스 플롯은 데이터의 최솟값, 1사분위수(Q1), 중앙값(Q2), 3사분위수(Q3), 최댓값을 시각적으로 보여주어 데이터의 분포와 중심 경향, 산포도를 한눈에 파악하게 해줍니다. 특히 IQR(Q3-Q1)을 기준으로 정상 범위를 벗어나는 데이터를 이상치로 쉽게 식별할 수 있습니다.

---

## 문제 41 (난이도: 보통)
인공 신경망의 기본 단위인 퍼셉트론(Perceptron)을 구성하는 세 가지 주요 요소로 올바르게 짝지어진 것은?

### 보기
A. 입력, 가중치, 활성화 함수
B. 필터, 스트라이드, 패딩
C. 쿼리, 키, 밸류
D. 인코더, 디코더, 컨텍스트 벡터
E. 상태, 행동, 보상

**정답:** A
**해설:** 퍼셉트론은 여러 개의 입력(input)을 받아 각각에 해당하는 가중치(weight)를 곱한 후 모두 합하고, 그 결과에 활성화 함수(activation function)를 적용하여 최종 출력값을 결정하는 구조로 이루어져 있습니다.
- B: CNN의 구성 요소입니다.
- C: 어텐션 메커니즘의 구성 요소입니다.
- D: Seq2Seq 모델의 구성 요소입니다.
- E: 강화 학습의 구성 요소입니다.

---

## 문제 42 (난이도: 보통)
딥러닝 모델에서, 입력값에 대한 가중합(weighted sum) 결과를 비선형(non-linear) 값으로 변환하여 모델의 표현력을 높여주는 함수는 무엇인가? (예: ReLU, Sigmoid, Tanh)

### 보기
A. 손실 함수 (Loss Function)
B. 옵티마이저 (Optimizer)
C. 활성화 함수 (Activation Function)
D. 커널 함수 (Kernel Function)
E. 비용 함수 (Cost Function)

**정답:** C
**해설:** 활성화 함수는 선형 연산만으로는 표현할 수 없는 복잡한 패턴을 학습할 수 있도록 신경망에 비선형성을 부여하는 매우 중요한 역할을 합니다. 만약 활성화 함수가 없다면, 신경망을 아무리 깊게 쌓아도 결국 하나의 선형 변환과 같아져 표현력에 한계가 생깁니다.

---

## 문제 43 (난이도: 보통)
BERT와 같은 인코더-온리(Encoder-Only) 트랜스포머 모델에 대한 설명으로 가장 적절한 것은?

### 보기
A. 텍스트 생성(generation)과 같은 자기회귀적(auto-regressive) 작업에 가장 적합하다.
B. 문장의 왼쪽에서 오른쪽으로만 정보를 처리하는 단방향(uni-directional) 모델이다.
C. 입력 문장의 일부를 마스킹(masking)하고 이를 예측하는 방식(Masked Language Model)으로 학습하여, 문장의 양방향 문맥을 깊게 이해한다.
D. 어텐션 메커니즘 없이 RNN 구조만을 사용하여 문맥을 이해한다.
E. 주로 이미지 분류(image classification) 작업에 사용된다.

**정답:** C
**해설:** BERT는 트랜스포머의 인코더 구조만을 사용하여, 입력 문장의 일부 단어를 `[MASK]` 토큰으로 바꾸고 주변 단어들을 이용해 원래 단어를 예측하도록 학습합니다. 이 과정에서 문장 전체의 왼쪽과 오른쪽 문맥을 모두 고려하므로, 문장에 대한 깊은 양방향 이해가 가능해져 분류나 개체명 인식과 같은 NLU(자연어 이해) 작업에서 뛰어난 성능을 보입니다.
- A: GPT와 같은 디코더 기반 모델의 특징입니다.

---

## 문제 44 (난이도: 보통)
LLM에게 별도의 파인튜닝 없이, 프롬프트에 몇 개의 예시(demonstration)를 함께 제공하여 원하는 작업을 수행하도록 유도하는 방식을 무엇이라고 하는가?

### 보기
A. 제로샷 학습 (Zero-shot Learning)
B. 퓨샷 학습 (Few-shot Learning)
C. 원샷 학습 (One-shot Learning)
D. 전이 학습 (Transfer Learning)
E. 메타 학습 (Meta-Learning)

**정답:** B
**해설:** 퓨샷 학습은 LLM의 창발적 능력 중 하나인 인-컨텍스트 학습(In-context Learning)을 활용하는 방식입니다. 모델에게 원하는 작업에 대한 몇 개의 예시(입력과 출력 쌍)를 보여줌으로써, 모델이 그 패턴을 파악하고 새로운 입력에 대해서도 동일한 작업을 수행하도록 유도합니다.
- A: 예시 없이 지시만으로 작업을 수행하게 하는 방식입니다.
- C: 단 하나의 예시만 제공하는 경우입니다.

---

## 문제 45 (난이도: 어려움)
LLM의 답변을 평가하는 방법 중 하나인 'LLM-as-a-judge'에 대한 설명으로 가장 옳지 않은 것은?

### 보기
A. 인간이 직접 평가하는 데 드는 비용과 시간을 줄이기 위해, 다른 강력한 LLM을 평가자로 사용하는 방식이다.
B. 평가의 일관성을 위해 상세한 평가 기준과 형식을 담은 프롬프트를 사용한다.
C. 평가 LLM이 특정 위치(예: 첫 번째)에 제시된 답변을 더 선호하는 '위치 편향(Position Bias)'이 존재할 수 있다.
D. 평가 LLM이 답변의 실제 품질과 상관없이 단순히 더 긴 답변에 높은 점수를 주는 '길이 편향(Length Bias)'이 나타날 수 있다.
E. LLM-as-a-judge는 인간 평가자와의 편향이 전혀 없고 항상 객관적인 평가를 보장한다.

**정답:** E
**해설:** LLM-as-a-judge는 매우 유용하지만, 여러 가지 편향(bias) 문제로부터 자유롭지 않습니다. 언급된 위치 편향, 길이 편향 외에도, 평가 모델 자신이 생성했을 법한 스타일의 답변을 더 선호하는 '자기 선호 편향(Self-preference Bias)' 등 다양한 편향이 존재하며, 이를 완화하기 위한 연구가 활발히 진행 중입니다.

---

## 문제 46 (난이도: 보통)
LLM의 주요 한계점 중 하나로, 사실과 다르거나 맥락에 맞지 않는 내용을 마치 사실인 것처럼 그럴듯하게 생성하는 현상을 무엇이라고 하는가?

### 보기
A. 탈옥 (Jailbreaking)
B. 편향 (Bias)
C. 환각 (Hallucination)
D. 과적합 (Overfitting)
E. 보상 해킹 (Reward Hacking)

**정답:** C
**해설:** 환각은 LLM이 학습 데이터에 없는 내용을 추론하거나, 잘못된 정보를 사실인 것처럼 자신감 있게 생성하는 현상을 말합니다. 이는 LLM의 신뢰성을 저해하는 가장 큰 문제 중 하나이며, RAG(검색 증강 생성)와 같은 기술이 이를 완화하기 위한 해결책으로 제시됩니다.

---

## 문제 47 (난이도: 보통)
RAG 파이프라인에서, 사용자의 질문(query)을 벡터로 변환한 뒤, 벡터 데이터베이스에 저장된 문서 청크 벡터들과 비교하여 의미적으로 가장 유사한 청크를 찾아주는 구성 요소를 무엇이라고 하는가?

### 보기
A. 로더 (Loader)
B. 스플리터 (Splitter)
C. 리트리버 (Retriever)
D. 파서 (Parser)
E. 체인 (Chain)

**정답:** C
**해설:** 리트리버는 RAG의 '검색(Retrieval)' 단계를 책임지는 핵심 구성 요소입니다. 사용자의 질문을 이해(임베딩)하고, 방대한 문서 벡터들 중에서 가장 관련성 높은 정보를 효율적으로 찾아내는 역할을 수행합니다. Sparse, Dense, Hybrid 등 다양한 방식의 리트리버가 있습니다.

---

## 문제 48 (난이도: 보통)
GPU 메모리가 부족하여 큰 배치 크기(batch size)를 사용할 수 없을 때, 여러 번의 작은 배치에서 계산된 기울기를 누적했다가 한 번에 가중치를 업데이트하여 큰 배치 크기의 효과를 내는 학습 기법은 무엇인가?

### 보기
A. 학습률 스케줄링 (Learning Rate Scheduling)
B. 조기 종료 (Early Stopping)
C. 그래디언트 클리핑 (Gradient Clipping)
D. 그래디언트 축적 (Gradient Accumulation)
E. 믹스드 프리시전 (Mixed Precision)

**정답:** D
**해설:** 그래디언트 축적은 메모리 제약 하에서 사실상의(effective) 배치 크기를 늘리는 매우 유용한 기법입니다. 예를 들어, 배치 크기 8로 4 스텝 동안 기울기를 축적하면, 배치 크기 32로 한 번 학습한 것과 유사한 효과를 얻을 수 있어 학습 안정성을 높일 수 있습니다.

---

## 문제 49 (난이도: 어려움)
이미지와 텍스트를 같은 임베딩 공간에 매핑하는 CLIP 모델에 대한 설명으로 가장 옳지 않은 것은?

### 보기
A. 대조 학습(Contrastive Learning)을 통해, 매칭되는 (이미지, 텍스트) 쌍의 임베딩은 서로 가까워지도록, 매칭되지 않는 쌍은 멀어지도록 학습한다.
B. 이미지 인코더와 텍스트 인코더라는 두 개의 독립적인 인코더를 사용하여 각 모달리티를 임베딩한다.
C. 특정 분류 작업에 대한 파인튜닝 없이도, 프롬프트(예: "a photo of a [CLASS]")를 이용해 처음 보는 클래스를 분류하는 제로샷(Zero-shot) 학습 능력이 뛰어나다.
D. 이미지 캡셔닝(Image Captioning)과 같이 복잡한 문장을 생성하는 작업에 직접적으로 사용된다.
E. CLIP의 성공은 이후 LLaVA와 같은 대규모 비전-언어 모델(LVLM)의 발전에 큰 영향을 주었다.

**정답:** D
**해설:** CLIP은 이미지와 텍스트 간의 유사도를 계산하여 가장 적절한 텍스트를 '선택'하거나, 이미지를 주어진 텍스트 후보군으로 '분류'하는 데 최적화된 모델입니다. 복잡한 문장을 처음부터 생성하는 이미지 캡셔닝과 같은 생성(generation) 작업은 CLIP의 주된 용도가 아니며, 이러한 작업에는 보통 인코더-디코더 구조의 모델이 사용됩니다.

---

## 문제 50 (난이도: 어려움)
모델 압축 기술인 양자화(Quantization)와 가지치기(Pruning)에 대한 비교 설명으로 가장 적절한 것은?

### 보기
A. 양자화는 가중치의 개수를, 가지치기는 가중치의 정밀도를 줄인다.
B. 양자화는 가중치의 정밀도를, 가지치기는 가중치의 개수를 줄인다.
C. 양자화는 항상 모델의 정확도를 높이지만, 가지치기는 항상 정확도를 낮춘다.
D. 가지치기는 모델의 메모리 사용량만 줄일 뿐, 추론 속도에는 영향을 주지 않는다.
E. 두 기술은 함께 사용할 수 없으며, 반드시 하나만 선택해야 한다.

**정답:** B
**해설:** 두 기술은 모델을 경량화하는 대표적인 방법이지만, 접근 방식이 다릅니다. 양자화는 각 가중치를 표현하는 비트 수(정밀도)를 줄여(예: 32비트→8비트) 모델 크기를 줄입니다. 가지치기는 중요도가 낮은 가중치(연결) 자체를 제거하여 모델의 전체 파라미터 개수를 줄입니다. 두 기술은 상호 보완적이며, 종종 함께 사용되어 압축 효과를 극대화합니다.
