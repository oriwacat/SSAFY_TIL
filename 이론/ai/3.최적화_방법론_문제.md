## 문제 1
다음 중 경사 하강법(Gradient Descent)에 대한 설명으로 가장 옳은 것은?

### 보기
A. 데이터의 숨겨진 구조를 파악하기 위해 정답이 없는 데이터를 사용한다.
B. 비용(Cost) 함수의 기울기(gradient)를 반복적으로 계산하고 그 기울기가 낮아지는 방향으로 파라미터를 업데이트하여 최적값을 찾는 방법이다.
C. 단 한 번의 계산으로 최적의 파라미터를 찾는다.
D. 데이터의 차원을 축소하여 계산 효율성을 높이는 데 중점을 둔다.
E. 보상과 벌칙을 통해 최적의 행동 정책을 학습한다.

**정답:** B
**해설:** 경사 하강법은 비용 함수의 기울기가 0에 가까워지는 지점, 즉 비용이 최소가 되는 지점을 찾기 위해 점진적으로 파라미터를 업데이트하는 최적화 알고리즘입니다.
**난이도:** 쉬움

---

## 문제 2
정규 방정식(Normal Equation)에 대한 설명으로 옳지 않은 것은?

### 보기
A. 경사 하강법과 달리 반복(iteration) 없이 한 번의 행렬 계산으로 최적의 해를 구한다.
B. 특성(feature)의 개수가 매우 많은 경우 경사 하강법보다 계산 속도가 빠르다.
C. 데이터셋의 역행렬이 존재하지 않으면 사용할 수 없다.
D. 최소 제곱법을 해석적으로 푸는 방법 중 하나이다.
E. 특이값 분해(SVD)와 같은 방법을 통해 역행렬이 없는 문제를 일부 해결할 수 있다.

**정답:** B
**해설:** 정규 방정식은 특성의 개수(n)가 커지면 역행렬 계산 비용(약 O(n^3))이 급격하게 증가하여 경사 하강법보다 느려집니다. 따라서 특성이 수천 개 이상인 대규모 데이터셋에서는 경사 하강법이 더 효율적입니다.
**난이도:** 보통

---

## 문제 3
경사 하강법에서 사용되는 하이퍼파라미터 '학습률(Learning Rate)'에 대한 설명으로 가장 적절한 것은?

### 보기
A. 학습률이 클수록 항상 더 빠르고 안정적으로 최적값에 수렴한다.
B. 학습률은 비용 함수의 기울기를 의미한다.
C. 학습률이 너무 작으면 최적값에 수렴하는 데 시간이 매우 오래 걸릴 수 있다.
D. 학습률은 전체 데이터셋을 몇 번 반복하여 학습할지를 결정하는 값이다.
E. 학습률은 한 번의 업데이트에 사용되는 데이터의 개수를 의미한다.

**정답:** C
**해설:** 학습률은 경사 하강법에서 파라미터를 업데이트하는 보폭(step size)을 결정합니다. 학습률이 너무 작으면 최적값에 도달하기까지 많은 반복이 필요하며, 반대로 너무 크면 최적값을 지나쳐 발산(divergence)할 수 있습니다.
**난이도:** 쉬움

---

## 문제 4
다음은 다양한 경사 하강법의 종류들입니다. 각 설명과 그에 해당하는 명칭이 바르게 짝지어진 것은?

### 보기
A. 전체 학습 데이터를 모두 사용하여 한 번에 파라미터를 업데이트한다. - 확률적 경사 하강법
B. 데이터 1개(샘플)를 무작위로 선택하여 파라미터를 업데이트한다. - 배치 경사 하강법
C. 전체 데이터를 작은 묶음(미니배치)으로 나누어 각 묶음마다 파라미터를 업데이트한다. - 미니배치 경사 하강법
D. 항상 가장 빠른 수렴 속도를 보장한다. - 배치 경사 하강법
E. 학습 과정이 가장 불안정하지만, 지역 최적점(local minima)을 탈출할 가능성이 있다. - 배치 경사 하강법

**정답:** C
**해설:** 미니배치 경사 하강법은 전체 데이터(배치)와 1개 데이터(확률적) 방식의 장점을 절충한 방식으로, 안정적이면서도 비교적 빠른 속도로 학습하여 현재 가장 널리 사용됩니다. 확률적 경사 하강법은 학습 과정이 불안정하지만 지역 최적점을 탈출하는 데 도움이 될 수 있습니다.
**난이도:** 보통

---

## 문제 5
경사 하강법의 학습 과정에서 사용되는 용어 '에포크(Epoch)'의 정확한 의미는 무엇인가요?

### 보기
A. 한 번의 파라미터 업데이트 과정.
B. 학습을 조기 종료하기 위한 오차의 임계값.
C. 전체 데이터셋을 처음부터 끝까지 한 번 모두 사용한 학습의 횟수.
D. 한 번의 업데이트에 사용되는 데이터 샘플의 묶음 크기.
E. 최적의 모델을 찾기 위한 반복적인 시도.

**정답:** C
**해설:** 1 에포크는 전체 학습 데이터셋이 경사 하강법 알고리즘을 통해 한 번 완전히 통과했음을 의미합니다. 예를 들어, 1000개의 데이터가 있고 배치 크기가 100이라면, 10번의 이터레이션(iteration)이 1 에포크가 됩니다.
**난이도:** 쉬움

---

## 문제 6
최소 제곱법(Least Squares Method)의 주된 목표는 무엇인가요?

### 보기
A. 모델의 복잡도를 최소화하는 것.
B. 데이터의 차원을 최소화하는 것.
C. 모델의 예측값과 실제 관측값 사이의 오차(잔차) 제곱의 합을 최소화하는 것.
D. 학습 데이터의 개수를 최소화하는 것.
E. 모델의 학습률을 자동으로 찾는 것.

**정답:** C
**해설:** 최소 제곱법은 회귀 모델에서 가장 널리 사용되는 원리 중 하나로, 모델이 데이터에 얼마나 잘 맞는지를 평가하는 척도인 '오차 제곱의 합'을 최소화하는 파라미터(가중치와 편향)를 찾는 것을 목표로 합니다.
**난이도:** 보통

---

## 문제 7
확률적 경사 하강법(Stochastic Gradient Descent, SGD)이 배치 경사 하강법(Batch Gradient Descent)에 비해 갖는 장점으로 옳은 것은?

### 보기
A. 학습 과정이 매우 안정적이다.
B. 항상 더 정확한 최적값으로 수렴한다.
C. 한 번의 파라미터 업데이트에 필요한 계산량이 적어 학습 속도가 빠를 수 있다.
D. 데이터셋의 모든 데이터를 항상 고려하므로 전역 최적점(global minimum)을 찾기 쉽다.
E. 학습률에 덜 민감하다.

**정답:** C
**해설:** SGD는 한 번에 하나의 데이터 샘플만 사용하므로, 전체 데이터를 사용하는 배치 경사 하강법보다 업데이트 속도가 훨씬 빠릅니다. 하지만 업데이트 방향의 변동성이 커서 학습 과정이 불안정할 수 있습니다.
**난이도:** 보통

---

## 문제 8
다음 중 경사 하강법 기반의 최신 최적화 알고리즘(Optimizer)이 아닌 것은?

### 보기
A. Adam
B. RMSProp
C. AdaGrad
D. K-평균(K-Means)
E. Nesterov Accelerated Gradient

**정답:** D
**해설:** K-평균은 데이터 포인트를 K개의 군집으로 묶는 비지도 학습의 군집화 알고리즘입니다. Adam, RMSProp, AdaGrad 등은 모두 경사 하강법을 개선하여 학습 속도와 성능을 높인 적응형(adaptive) 최적화 알고리즘들입니다.
**난이도:** 쉬움

---

## 문제 9
경사 하강법에서 '배치 크기(Batch Size)'를 조절할 때 발생하는 현상에 대한 설명으로 옳지 않은 것은?

### 보기
A. 배치 크기를 매우 크게 하면(전체 데이터 사용) 메모리 사용량이 늘어난다.
B. 배치 크기를 1로 하면 확률적 경사 하강법과 같아진다.
C. 배치 크기가 작을수록 학습 과정의 안정성은 높아진다.
D. 배치 크기가 클수록 그래디언트 추정의 정확도가 높아진다.
E. 적절한 배치 크기는 하드웨어(GPU 등)의 병렬 처리 능력을 활용하여 학습 속도를 높일 수 있다.

**정답:** C
**해설:** 배치 크기가 작을수록(특히 1에 가까울수록) 각 업데이트 단계에서 사용하는 데이터의 노이즈가 심해져 학습 과정이 불안정해집니다. 반대로 배치 크기가 클수록 전체 데이터의 경향을 더 잘 반영하므로 학습 과정이 안정됩니다.
**난이도:** 보통

---

## 문제 10
모델 학습 시 비용 함수가 더 이상 감소하지 않고 특정 값 주변에서 진동하거나, 오히려 증가하는 현상이 발생했습니다. 다음 중 가장 가능성 있는 원인과 해결책은 무엇인가요?

### 보기
A. 원인: 학습률이 너무 작다. / 해결책: 학습률을 더 크게 조절한다.
B. 원인: 학습률이 너무 크다. / 해결책: 학습률을 더 작게 조절하거나, 학습률 스케줄링을 적용한다.
C. 원인: 배치 크기가 너무 크다. / 해결책: 배치 크기를 1로 고정한다.
D. 원인: 에포크(Epoch) 수가 부족하다. / 해결책: 에포크 수를 줄인다.
E. 원인: 데이터가 너무 많다. / 해결책: 데이터를 추가로 수집한다.

**정답:** B
**해설:** 학습률이 너무 크면 파라미터 업데이트 폭이 커져 최적점을 지나쳐 버리고, 이로 인해 비용 함수가 수렴하지 못하고 진동하거나 발산(증가)할 수 있습니다. 이 경우 학습률을 줄여 업데이트 보폭을 작게 만들거나, 학습이 진행됨에 따라 학습률을 점차 줄이는 학습률 스케줄링(Learning Rate Scheduling)을 적용하는 것이 효과적입니다.
**난이도:** 보통
