## 문제 1 (난이도: 쉬움)
다음 머신러닝의 학습 종류 중, 데이터에 정답(label)이 주어져 있어 모델이 입력과 정답 사이의 관계를 학습하는 방식은 무엇인가?

### 보기
A. 비지도 학습 (Unsupervised Learning)
B. 강화 학습 (Reinforcement Learning)
C. 지도 학습 (Supervised Learning)
D. 자기 지도 학습 (Self-supervised Learning)
E. 전이 학습 (Transfer Learning)

**정답:** C
**해설:** 지도 학습은 정답이 있는 데이터를 사용하여 모델을 학습시키는 방법입니다. 모델은 입력 데이터를 보고 정답을 예측하는 방법을 배우며, 회귀(Regression)와 분류(Classification)가 대표적인 예입니다.
- A: 정답 없는 데이터에서 패턴이나 구조를 학습합니다.
- B: 보상과 벌을 통해 행동을 학습합니다.
- D: 데이터 자체에서 정답을 만들어내 학습하는 방식입니다.
- E: 한 문제에서 학습된 모델을 다른 문제에 재사용하는 기법입니다.

---

## 문제 2 (난이도: 보통)
모델의 손실 함수(Loss Function)를 최소화하기 위해, 기울기(gradient)의 반대 방향으로 파라미터를 점진적으로 업데이트해 나가는 최적화 알고리즘은 무엇인가?

### 보기
A. 정규 방정식 (Normal Equation)
B. 최소 제곱법 (Least Squares Method)
C. K-평균 군집화 (K-Means Clustering)
D. 주성분 분석 (PCA)
E. 경사 하강법 (Gradient Descent)

**정답:** E
**해설:** 경사 하강법은 손실 함수의 기울기가 가장 가파른 방향의 반대쪽으로 파라미터를 조금씩 이동시켜, 점진적으로 손실이 가장 낮은 지점(최적점)을 찾아가는 반복적인 최적화 방법입니다. 딥러닝에서 가장 기본이 되는 최적화 원리입니다.
- A: 역행렬을 이용해 한 번에 최적해를 구하는 해석적 방법입니다.
- B: 오차의 제곱합을 최소화하는 원리 자체를 의미합니다.
- C, D: 비지도 학습 알고리즘입니다.

---

## 문제 3 (난이도: 쉬움)
데이터 탐색(EDA) 과정에서, 두 변수 간의 관계가 얼마나 강한지를 -1과 1 사이의 값으로 나타내는 지표는 무엇인가? (단, 이 지표가 높다고 해서 인과관계가 성립하는 것은 아니다.)

### 보기
A. 평균 제곱 오차 (MSE)
B. 상관계수 (Correlation Coefficient)
C. 사분위수 범위 (IQR)
D. 정확도 (Accuracy)
E. P-값 (P-value)

**정답:** B
**해설:** 상관계수는 두 변수가 함께 변화하는 정도를 나타내는 통계적 척도입니다. 1에 가까울수록 강한 양의 상관관계, -1에 가까울수록 강한 음의 상관관계를 의미하며, 0에 가까우면 선형적인 관계가 없음을 의미합니다. 중요한 점은 상관관계가 인과관계를 의미하지는 않는다는 것입니다.
- A, D: 모델 성능 평가 지표입니다.
- C: 데이터의 산포도를 나타내는 값입니다.
- E: 가설 검정에서 사용되는 통계량입니다.

---

## 문제 4 (난이도: 보통)
모델이 훈련 데이터에만 과도하게 최적화되어, 학습 시에는 성능이 높지만 새로운 테스트 데이터에 대해서는 성능이 현저히 낮아지는 현상은 무엇인가?

### 보기
A. 과소적합 (Underfitting)
B. 과적합 (Overfitting)
C. 데이터 유출 (Data Leakage)
D. 일반화 (Generalization)
E. 경사 소실 (Gradient Vanishing)

**정답:** B
**해설:** 과적합은 모델이 훈련 데이터의 노이즈나 특정 패턴까지 모두 외워버려서, 새로운 데이터에 대한 일반화 성능이 떨어지는 현상을 말합니다. 모델의 복잡도가 너무 높거나 훈련 데이터가 부족할 때 주로 발생합니다.
- A: 모델이 너무 단순하여 훈련 데이터의 패턴조차 제대로 학습하지 못하는 현상입니다.
- C: 훈련 시 사용되어서는 안 될 정보가 모델에 노출되는 것입니다.
- D: 모델이 새로운 데이터에 대해 정확하게 예측하는 능력으로, 과적합은 일반화 성능이 낮은 상태입니다.

---

## 문제 5 (난이도: 보통)
데이터의 특성(feature)마다 값의 범위(scale)가 다를 때 발생하는 문제를 해결하기 위해, 모든 데이터 포인트의 평균을 0, 표준편차를 1로 변환하는 전처리 기법은 무엇인가?

### 보기
A. 정규화 (Normalization)
B. 표준화 (Standardization)
C. 원-핫 인코딩 (One-Hot Encoding)
D. 차원 축소 (Dimensionality Reduction)
E. 이상치 제거 (Outlier Removal)

**정답:** B
**해설:** 표준화는 각 특성의 스케일을 통일하여 모든 특성이 모델 학습에 공평하게 영향을 미치도록 하는 기법입니다. 특히 거리 기반 알고리즘(예: SVM)이나 경사 하강법을 사용하는 모델에서 성능 향상에 중요한 역할을 합니다. 정규화(Normalization)와는 달리 이상치에 덜 민감한 특징이 있습니다.
- A: 데이터의 값을 0과 1 사이로 변환하는 기법입니다.

---

## 문제 6 (난이도: 보통)
분류 모델의 성능 평가 지표 중, 실제 Positive인 데이터 중에서 모델이 Positive라고 올바르게 예측한 비율을 무엇이라고 하는가? (이 지표는 암 진단과 같이 실제 Positive를 놓치면 안 되는 경우에 특히 중요하다.)

### 보기
A. 정확도 (Accuracy)
B. 정밀도 (Precision)
C. F1-점수 (F1-Score)
D. 재현율 (Recall)
E. 특이도 (Specificity)

**정답:** D
**해설:** 재현율(Recall) 또는 민감도(Sensitivity)는 실제 참(Positive)인 것들 중 모델이 얼마나 많이 참으로 예측했는지를 나타냅니다. 즉, 모델이 '놓치지 않고 얼마나 잘 찾아내는가'를 측정하는 지표입니다.
- B: 모델이 참이라고 예측한 것들 중 실제 참의 비율입니다.

---

## 문제 7 (난이도: 보통)
데이터셋이 제한적일 때 모델의 일반화 성능을 더 신뢰성 있게 평가하기 위해, 훈련 데이터를 여러 개(K개)의 폴드(fold)로 나누어 각 폴드가 한 번씩 검증 데이터 역할을 하도록 반복적으로 평가하는 방법은 무엇인가?

### 보기
A. 홀드아웃 검증 (Hold-out Validation)
B. 부트스트래핑 (Bootstrapping)
C. K-폴드 교차 검증 (K-Fold Cross-Validation)
D. 그리드 서치 (Grid Search)
E. 랜덤 서치 (Random Search)

**정답:** C
**해설:** K-폴드 교차 검증은 데이터를 어떻게 나누느냐에 따라 모델의 성능 평가가 달라지는 것을 방지하기 위한 기법입니다. 전체 훈련 데이터를 K개의 부분집합으로 나누고, K번의 학습과 평가를 반복하며 각 부분집합이 한 번씩 검증 세트로 사용됩니다. 이후 K개의 성능 점수를 평균 내어 최종 성능을 측정합니다.
- A: 데이터를 훈련/검증/테스트 세트로 한 번만 나누는 방식입니다.
- D, E: 하이퍼파라미터 튜닝 기법입니다.

---

## 문제 8 (난이도: 보통)
딥러닝 모델의 학습 과정에서, 손실(loss)을 기반으로 출력층에서부터 입력층 방향으로 각 가중치에 대한 기울기를 효율적으로 계산하는 알고리즘은 무엇인가?

### 보기
A. 순전파 (Forward Propagation)
B. 역전파 (Backpropagation)
C. 경사 하강법 (Gradient Descent)
D. 드롭아웃 (Dropout)
E. 활성화 함수 (Activation Function)

**정답:** B
**해설:** 역전파 알고리즘은 미분의 연쇄 법칙(Chain Rule)을 활용하여, 출력층의 손실로부터 시작해 거꾸로 한 단계씩 미분값을 계산하며 전체 파라미터에 대한 기울기를 효율적으로 구하는 방법입니다. 이 기울기 값을 경사 하강법에 사용하여 가중치를 업데이트합니다.
- A: 입력이 출력으로 전달되는 과정입니다.
- C: 역전파로 구한 기울기를 사용해 가중치를 업데이트하는 최적화 방법입니다.

---

## 문제 9 (난이도: 보통)
과적합을 방지하기 위한 규제(Regularization) 기법 중, 학습 과정에서 신경망의 일부 뉴런을 무작위로 비활성화하여 모델이 특정 뉴런에 과도하게 의존하는 것을 막는 기법은 무엇인가?

### 보기
A. L1 규제 (L1 Regularization)
B. L2 규제 (L2 Regularization)
C. 조기 종료 (Early Stopping)
D. 드롭아웃 (Dropout)
E. 데이터 증강 (Data Augmentation)

**정답:** D
**해설:** 드롭아웃은 각 학습 스텝마다 지정된 비율만큼의 뉴런을 무작위로 '끄는' 기법입니다. 이는 마치 매번 다른 구조의 작은 모델 여러 개를 학습시켜 앙상블하는 것과 유사한 효과를 내어, 모델의 일반화 성능을 높이고 과적합을 효과적으로 방지합니다.

---

## 문제 10 (난이도: 보통)
자연어 처리에서 단어를 벡터로 표현할 때, 단어의 의미적 유사성을 벡터 공간에서의 거리로 표현할 수 있는 밀집 벡터(dense vector) 표현법은 무엇인가? (예: '왕' - '남자' + '여자' ≈ '여왕')

### 보기
A. 원-핫 인코딩 (One-Hot Encoding)
B. TF-IDF
C. N-gram
D. Word2Vec
E. Bag-of-Words (BoW)

**정답:** D
**해설:** Word2Vec은 단어의 의미를 다차원 공간의 밀집 벡터(임베딩)로 표현하는 기법입니다. 주변 단어와의 관계를 기반으로 학습하여, 의미적으로 유사한 단어들이 벡터 공간에서 가깝게 위치하게 됩니다. 이로 인해 단어 간의 의미론적, 문법적 관계를 벡터 연산으로 표현할 수 있습니다.
- A, E: 단어의 등장 유무나 빈도만 표현하며, 의미적 유사성을 담지 못합니다.

---

## 문제 11 (난이도: 보통)
순환 신경망(RNN)이 긴 시퀀스를 처리할 때, 정보가 뒤로 전달될수록 기울기가 점차 사라져 장기 의존성(Long-term dependency) 학습이 어려워지는 고질적인 문제는 무엇인가?

### 보기
A. 차원의 저주 (Curse of Dimensionality)
B. 기울기 소실 문제 (Vanishing Gradient Problem)
C. 과적합 문제 (Overfitting Problem)
D. 병목 현상 (Bottleneck Problem)
E. 계산 복잡성 증가 (Increased Computational Complexity)

**정답:** B
**해설:** RNN은 역전파 과정에서 동일한 가중치 행렬을 시퀀스의 길이만큼 반복적으로 곱하게 됩니다. 이때 행렬의 특정 값들이 1보다 작으면, 계속된 곱셈으로 인해 기울기가 0에 가까워져 사라지게 됩니다. 이로 인해 모델이 시퀀스의 앞부분에 있는 오래된 정보를 학습하지 못하는 문제가 발생합니다.

---

## 문제 12 (난이도: 보통)
RNN의 기울기 소실 문제를 해결하기 위해, 셀 상태(Cell State)와 망각/입력/출력 게이트(Gate)를 도입하여 정보의 흐름을 제어하는 개선된 순환 신경망 모델은 무엇인가?

### 보기
A. MLP (Multi-Layer Perceptron)
B. CNN (Convolutional Neural Network)
C. Autoencoder
D. Transformer
E. LSTM (Long Short-Term Memory)

**정답:** E
**해설:** LSTM은 RNN의 내부에 컨베이어 벨트와 같은 역할을 하는 '셀 상태'를 추가하고, 세 개의 게이트(망각, 입력, 출력)를 통해 이 셀 상태에 어떤 정보를 남기고, 추가하고, 출력할지를 동적으로 제어합니다. 이를 통해 중요한 정보를 오래 기억하고 불필요한 정보는 잊어버릴 수 있어 장기 의존성 문제를 효과적으로 해결합니다.

---

## 문제 13 (난이도: 보통)
Seq2Seq 모델에서 입력 시퀀스의 모든 정보를 고정된 크기의 벡터 하나(컨텍스트 벡터)에 압축할 때 발생하는 정보 손실 문제(Bottleneck Problem)를 해결하기 위해 등장한 메커니즘은 무엇인가?

### 보기
A. 어텐션 (Attention)
B. 드롭아웃 (Dropout)
C. 배치 정규화 (Batch Normalization)
D. 잔여 연결 (Residual Connection)
E. 교차 검증 (Cross-Validation)

**정답:** A
**해설:** 어텐션 메커니즘은 디코더가 각 타임스텝에서 단어를 생성할 때마다, 인코더의 모든 은닉 상태를 다시 참조하여 현재 생성할 단어와 가장 관련이 깊은 입력 단어에 더 높은 가중치(attention)를 부여합니다. 이를 통해 고정된 컨텍스트 벡터에만 의존하지 않고, 필요할 때마다 입력 시퀀스의 중요한 정보를 직접 가져와 사용하여 성능을 크게 향상시켰습니다.

---

## 문제 14 (난이도: 보통)
트랜스포머(Transformer)의 핵심 아이디어로, RNN의 순차적 처리 방식에서 벗어나 시퀀스 내의 모든 단어 간의 관계를 한 번에 병렬적으로 계산하는 메커니즘은 무엇인가?

### 보기
A. 순환 구조 (Recurrent Structure)
B. 합성곱 (Convolution)
C. 셀프 어텐션 (Self-Attention)
D. 풀링 (Pooling)
E. 완전 연결 (Fully Connection)

**정답:** C
**해설:** 셀프 어텐션은 하나의 시퀀스 내에서 각 단어가 다른 모든 단어와 얼마나 관련이 있는지를 Query, Key, Value 벡터를 이용해 직접 계산합니다. 이 과정은 재귀적인 구조 없이 행렬 연산으로 한 번에 처리될 수 있어 병렬화에 매우 유리하며, 이를 통해 트랜스포머는 RNN보다 훨씬 빠르고 효율적으로 장거리 의존성을 학습할 수 있습니다.

---

## 문제 15 (난이도: 보통)
CNN(합성곱 신경망)에서 이미지의 지역적 특징(local feature)을 추출하기 위해 사용되는 핵심적인 연산은 무엇인가?

### 보기
A. 풀링 (Pooling)
B. 합성곱 (Convolution)
C. 완전 연결 (Fully Connection)
D. 드롭아웃 (Dropout)
E. 역전파 (Backpropagation)

**정답:** B
**해설:** 합성곱 연산은 필터(커널)를 이미지 위에서 이동시키며 지역적인 패턴(모서리, 질감 등)을 감지하고 특징 맵(Feature Map)을 생성하는 과정입니다. 이는 이미지 전체를 한 번에 처리하는 대신 지역 정보를 효과적으로 추출하는 CNN의 핵심 원리입니다.

---

## 문제 16 (난이도: 보통)
CNN에서 합성곱 층을 통과한 특징 맵의 크기를 줄여 계산량을 감소시키고, 약간의 위치 변화에도 모델이 강건하게 반응하도록 만드는 연산은 무엇인가?

### 보기
A. 스트라이드 (Stride)
B. 패딩 (Padding)
C. 풀링 (Pooling)
D. 정규화 (Normalization)
E. 활성화 함수 (Activation Function)

**정답:** C
**해설:** 풀링(주로 Max Pooling)은 특징 맵의 특정 영역에서 가장 중요한 특징(최댓값)만을 남기고 나머지는 버리는 다운샘플링(down-sampling) 과정입니다. 이를 통해 데이터의 크기를 줄여 연산을 효율화하고, 사소한 변화에 덜 민감한, 더 강건한 특징을 추출할 수 있습니다.

---

## 문제 17 (난이도: 보통)
네트워크가 매우 깊어질 때 발생하는 성능 저하 문제(Degradation Problem)를 해결하기 위해, 입력값을 출력값에 그대로 더해주는 '스킵 연결(Skip Connection)' 구조를 도입한 혁신적인 CNN 아키텍처는 무엇인가?

### 보기
A. VGGNet
B. AlexNet
C. GoogleNet
D. ResNet
E. MobileNet

**정답:** D
**해설:** ResNet(Residual Network)은 잔차 연결(Residual Connection) 또는 스킵 연결이라 불리는 구조를 통해, 층이 깊어지더라도 학습이 용이하도록 만들었습니다. 이는 기울기 소실 문제를 완화하고, 이전까지 불가능했던 수백, 수천 층의 매우 깊은 신경망을 성공적으로 학습시키는 길을 열었습니다.

---

## 문제 18 (난이도: 보통)
자연어 처리에서 성공한 트랜스포머 구조를 컴퓨터 비전 분야에 적용하여, 이미지를 여러 개의 패치(patch)로 나누고 이를 단어 시퀀스처럼 처리하는 새로운 패러다임을 제시한 모델은 무엇인가?

### 보기
A. ResNet
B. EfficientNet
C. ViT (Vision Transformer)
D. FCN (Fully Convolutional Network)
E. YOLO

**정답:** C
**해설:** ViT는 CNN의 합성곱 연산 대신, 이미지를 16x16 같은 고정된 크기의 패치로 분할하고 각 패치를 트랜스포머의 입력 토큰으로 사용합니다. 이를 통해 셀프 어텐션 메커니즘으로 이미지 전체의 전역적인 관계를 학습하며, 대규모 데이터셋에서 CNN을 능가하는 성능을 보여주었습니다.

---

## 문제 19 (난이도: 보통)
거대 언어 모델(LLM)의 규모가 커질수록 성능이 예측 가능하게 향상된다는 법칙과, 특정 규모를 넘어서면서 질적으로 다른 새로운 능력이 갑자기 나타나는 현상을 각각 올바르게 짝지은 것은?

### 보기
A. 창발성, 규모의 법칙
B. 규모의 법칙, 창발성
C. 무어의 법칙, 창발성
D. 규모의 법칙, 과적합
E. 인-컨텍스트 학습, 규모의 법칙

**정답:** B
**해설:** 규모의 법칙(Scaling Law)은 모델 크기, 데이터셋 크기, 학습 계산량이라는 세 가지 요소가 커질수록 모델의 손실(loss)이 예측 가능한 방식으로 감소한다는 것을 의미합니다. 창발성(Emergent Property)은 이러한 양적 증가가 계속되다 보면, 특정 규모에서 예측하지 못했던 질적인 변화, 즉 새로운 능력(예: 인-컨텍스트 학습, 복잡한 추론)이 갑자기 발현되는 현상을 말합니다.

---

## 문제 20 (난이도: 보통)
LLM이 사전 학습 과정에서 배운 지식만으로는 답변할 수 없는 최신 정보나 특정 도메인의 전문 지식을 답변에 활용하기 위해, 외부 데이터베이스에서 관련 문서를 검색(Retrieve)하여 LLM의 입력 프롬프트에 함께 제공하는 기법은 무엇인가?

### 보기
A. 파인튜닝 (Fine-tuning)
B. 프롬프트 엔지니어링 (Prompt Engineering)
C. 검색 증강 생성 (RAG, Retrieval-Augmented Generation)
D. 지식 증류 (Knowledge Distillation)
E. 인-컨텍스트 학습 (In-context Learning)

**정답:** C
**해설:** RAG는 LLM의 한계인 '환각(Hallucination)' 현상과 '지식 단절(Knowledge Cut-off)' 문제를 해결하기 위한 강력한 기법입니다. 사용자의 질문이 들어오면, 먼저 외부의 신뢰할 수 있는 데이터 소스(Vector DB 등)에서 관련 정보를 검색하고, 이 정보를 LLM의 컨텍스트에 추가하여 더 정확하고 사실에 기반한 답변을 생성하도록 유도합니다.

---

## 문제 21 (난이도: 보통)
ChatGPT의 핵심 기술로, 모델이 생성한 여러 답변에 대해 인간이 직접 선호도 순위를 매기고, 이 데이터를 학습한 '보상 모델'을 통해 LLM이 더 인간의 선호에 맞는 답변을 생성하도록 강화학습을 수행하는 정렬(Alignment) 기법은 무엇인가?

### 보기
A. 지시 학습 (Instruction Tuning)
B. 제로샷 학습 (Zero-shot Learning)
C. 자기 지도 학습 (Self-supervised Learning)
D. RLHF (Reinforcement Learning from Human Feedback)
E. 전이 학습 (Transfer Learning)

**정답:** D
**해설:** RLHF는 '좋은 답변'이라는 주관적이고 복잡한 개념을 모델이 학습하도록 하는 획기적인 방법입니다. (1) 지도 파인튜닝, (2) 인간의 선호 데이터를 이용한 보상 모델 학습, (3) 보상 모델을 기준으로 한 강화학습의 3단계 과정을 통해, 모델이 더 유용하고, 무해하며, 진실된 답변을 생성하도록 정교하게 정렬합니다.

---

## 문제 22 (난이도: 보통)
LLM이 텍스트를 생성할 때, 확률적으로 가장 가능성 있는 상위 K개의 토큰들 중에서만 다음 토큰을 샘플링하여, 생성되는 텍스트의 품질과 다양성 사이의 균형을 맞추는 디코딩 전략은 무엇인가?

### 보기
A. Greedy Decoding
B. Beam Search
C. Top-K Sampling
D. Top-P (Nucleus) Sampling
E. Temperature Sampling

**정답:** C
**해설:** Top-K 샘플링은 확률이 매우 낮은 이상한 토큰들이 선택되는 것을 방지하면서도, Greedy 방식처럼 단 하나의 선택지에 얽매이지 않도록 하는 방법입니다. 확률 분포에서 가장 확률이 높은 K개의 토큰으로 후보군을 제한하고, 그 안에서 확률에 비례하여 다음 토큰을 랜덤하게 선택하여 적절한 수준의 일관성과 다양성을 모두 확보합니다.

---

## 문제 23 (난이도: 보통)
RAG 시스템에서, 키워드의 등장 빈도에 기반한 전통적인 검색(Sparse Retrieval)과 문장의 의미적 유사도에 기반한 벡터 검색(Dense Retrieval)을 함께 사용하여 검색 성능을 극대화하는 접근 방식을 무엇이라고 하는가?

### 보기
A. 교차 인코더 검색 (Cross-encoder Search)
B. 이중 인코더 검색 (Bi-encoder Search)
C. 하이브리드 검색 (Hybrid Search)
D. 계층적 검색 (Hierarchical Search)
E. 그래프 기반 검색 (Graph-based Search)

**정답:** C
**해설:** 하이브리드 검색은 Sparse Retriever(예: TF-IDF, BM25)와 Dense Retriever(벡터 검색)의 장점을 결합한 방식입니다. Sparse 검색은 키워드 정확성이 높고, Dense 검색은 의미적 유연성이 높기 때문에, 두 방식의 점수를 결합하여 순위를 매기면 각각의 단점을 보완하고 전반적인 검색 관련성을 크게 향상시킬 수 있습니다.

---

## 문제 24 (난이도: 보통)
Dense Retriever에서 사용되는 아키텍처 중, 질문(Query)과 문서(Document)를 독립적으로 각각 임베딩한 후 코사인 유사도 등으로 빠르게 비교하는 방식은 무엇인가? (이 방식은 속도가 빠르지만, 둘 간의 상호작용을 직접 보지 못하는 단점이 있다.)

### 보기
A. Cross-Encoder
B. Bi-Encoder
C. Auto-Encoder
D. Variational Auto-Encoder
E. Fusion-in-Decoder

**정답:** B
**해설:** Bi-Encoder(또는 Twin-Encoder) 구조는 이름처럼 두 개의 독립된 인코더(보통 동일한 가중치를 공유)를 사용하여 질문과 문서를 각각의 벡터로 변환합니다. 문서들은 미리 벡터로 변환하여 저장해둘 수 있으므로, 검색 시점에는 질문만 인코딩하여 저장된 벡터들과 빠르게 비교하면 되기 때문에 대규모 문서 검색에 매우 효율적입니다.
- A: 질문과 문서를 쌍으로 묶어 하나의 인코더에 입력하므로 속도는 느리지만 정확도가 높습니다.

---

## 문제 25 (난이도: 쉬움)
수십억 개의 파라미터를 가진 LLM 전체를 파인튜닝하는 대신, 기존 모델의 가중치는 고정한 채 소수의 파라미터만 추가하여 학습함으로써 계산 효율을 크게 높이는 PEFT(Parameter-Efficient Fine-Tuning)의 대표적인 기법은 무엇인가?

### 보기
A. 양자화 (Quantization)
B. 가지치기 (Pruning)
C. LoRA (Low-Rank Adaptation)
D. 지식 증류 (Knowledge Distillation)
E. 전이 학습 (Transfer Learning)

**정답:** C
**해설:** LoRA는 PEFT 기법 중 가장 성공적이고 널리 사용되는 방법 중 하나입니다. 사전 학습된 가중치의 변화량이 저-랭크(low-rank) 행렬로 근사될 수 있다는 아이디어에 기반하여, 원래의 가중치는 그대로 두고 저-랭크 행렬에 해당하는 매우 적은 수의 파라미터만 학습하여 높은 효율과 준수한 성능을 동시에 달성합니다.

---

## 문제 26 (난이도: 보통)
LoRA 기법과 4비트 양자화(Quantization)를 결합하여, 일반 소비자용 GPU에서도 거대 언어 모델의 파인튜닝이 가능하도록 메모리 사용량을 획기적으로 줄인 기술은 무엇인가?

### 보기
A. AdaLoRA
B. LongLoRA
C. QLoRA
D. SparseLoRA
E. Unsloth

**정답:** C
**해설:** QLoRA는 사전 학습된 모델의 가중치를 4비트로 양자화하여 메모리에 로드하고, 이 양자화된 모델 위에 LoRA 어댑터를 추가하여 파인튜닝을 진행하는 방식입니다. 이를 통해 16비트 모델을 직접 로드할 때보다 메모리 요구량을 극적으로 줄여, 제한된 하드웨어 환경에서도 LLM 파인튜닝의 접근성을 크게 높였습니다.

---

## 문제 27 (난이도: 보통)
모델의 가중치나 활성화 값을 더 낮은 정밀도의 데이터 타입(예: FP32 → FP16 또는 INT8)으로 변환하여 모델의 크기와 메모리 사용량을 줄이고, 특정 하드웨어에서 추론 속도를 높이는 모델 압축 기술은 무엇인가?

### 보기
A. 가지치기 (Pruning)
B. 양자화 (Quantization)
C. 지식 증류 (Knowledge Distillation)
D. 드롭아웃 (Dropout)
E. 정규화 (Normalization)

**정답:** B
**해설:** 양자화는 모델의 파라미터를 표현하는 데 필요한 비트 수를 줄이는 기술의 총칭입니다. 예를 들어, 32비트 부동소수점으로 저장된 가중치를 8비트 정수로 변환하면 모델 크기는 1/4로 줄어들고, 정수 연산에 최적화된 하드웨어에서는 추론 속도가 빨라지는 효과를 얻을 수 있습니다.

---

## 문제 28 (난이도: 보통)
양자화(Quantization)를 적용하는 시점에 따라 기법을 분류할 때, 이미 학습된 모델을 가져와 재학습 없이 양자화를 적용하는 간편한 방식과, 학습 과정 중에 양자화로 인한 오차를 함께 시뮬레이션하며 학습하여 정확도 저하를 최소화하는 방식을 각각 올바르게 짝지은 것은?

### 보기
A. QAT, PTQ
B. PTQ, QAT
C. 정적 양자화, 동적 양자화
D. 동적 양자화, 정적 양자화
E. 가중치 전용 양자화, 가중치-활성화 양자화

**정답:** B
**해설:** PTQ(Post-Training Quantization)는 학습이 끝난 후에 적용하는 방식으로, 빠르고 간편하지만 정확도 손실이 클 수 있습니다. 반면, QAT(Quantization-Aware Training)는 학습 과정에서부터 양자화를 고려하여 모델이 오차에 적응하도록 훈련하므로, 과정은 복잡하지만 정확도 손실을 최소화할 수 있습니다.

---

## 문제 29 (난이도: 보통)
모델의 성능에 거의 영향을 주지 않는 작은 값의 가중치 연결을 영구적으로 제거하여, 모델의 파라미터 수를 줄이고 희소(sparse) 모델로 만드는 압축 기술은 무엇인가?

### 보기
A. 양자화 (Quantization)
B. 드롭아웃 (Dropout)
C. 가중치 감쇠 (Weight Decay)
D. 가지치기 (Pruning)
E. 지식 증류 (Knowledge Distillation)

**정답:** D
**해설:** 가지치기(Pruning)는 모델의 복잡도를 줄이는 효과적인 방법 중 하나입니다. 중요도가 낮은 가중치를 제거함으로써 모델의 크기를 줄이고, 경우에 따라서는 추론 속도를 향상시킬 수 있습니다. 제거하는 단위에 따라 비정형(Unstructured) 가지치기와 정형(Structured) 가지치기로 나뉩니다.

---

## 문제 30 (난이도: 보통)
크고 복잡한 '교사 모델'이 가진 지식(예: 클래스 간의 미묘한 확률 분포)을 작고 효율적인 '학생 모델'이 학습하도록 하여, 모델의 크기는 줄이면서도 성능은 최대한 보존하려는 모델 압축 기법은 무엇인가?

### 보기
A. 전이 학습 (Transfer Learning)
B. 지식 증류 (Knowledge Distillation)
C. 메타 학습 (Meta-Learning)
D. 자기 지도 학습 (Self-supervised Learning)
E. 앙상블 학습 (Ensemble Learning)

**정답:** B
**해설:** 지식 증류는 교사 모델의 'soft labels'(예: [0.1, 0.2, 0.7])를 학생 모델의 학습 타겟으로 사용하여, 정답인 'hard label'([0, 0, 1])만 사용하는 것보다 더 풍부한 정보를 전달하는 방식입니다. 이를 통해 학생 모델은 더 적은 파라미터로도 교사 모델과 유사한 일반화 성능을 달성할 수 있습니다.
