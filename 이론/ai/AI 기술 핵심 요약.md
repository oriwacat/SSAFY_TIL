# AI 기술 핵심 요약

이 문서는 AI 기술의 핵심 개념들을 요약 정리합니다.

---

## 1. 머신러닝 기초

### 1.1. 학습 종류
- **지도 학습 (Supervised Learning)**
    - 정답지가 있는 데이터로 학습
    - **회귀 (Regression)**: 연속적인 숫자 값을 예측 (예: 선형 회귀)
    - **분류 (Classification)**: 데이터를 정해진 범주로 나눔
        - **로지스틱 회귀**: 이진 분류. 선형 회귀에 시그모이드 함수와 이진 교차 엔트로피 손실 함수를 사용.
        - **소프트맥스 회귀**: 다중 분류. 선형 회귀에 소프트맥스 함수와 범주형 교차 엔트로피 손실 함수를 사용.
- **비지도 학습 (Unsupervised Learning)**
    - 정답지가 없는 데이터로 학습
    - **군집화 (Clustering)**: 비슷한 데이터끼리 그룹으로 묶음 (예: K-Means, 계층적 군집)
    - **차원 축소 (Dimensionality Reduction)**: 데이터의 복잡성을 줄여 핵심 특징만 남김. '차원의 저주' 문제 해결에 도움. (예: PCA)
- **강화 학습 (Reinforcement Learning)**
    - 에이전트가 환경과 상호작용하며 상과 벌을 통해 최적의 행동을 학습.

### 1.2. 최적화 원리
- **최소 제곱법 (Least Squares Method)**
    - 예측 모델의 비용(오차 제곱의 합)을 최소화하는 파라미터를 찾는 원리.
- **경사 하강법 (Gradient Descent)**
    - 비용 함수의 기울기 반대 방향으로 파라미터를 점진적으로 업데이트하여 최적점을 찾아가는 방법.
    - **주요 개념**: 이터레이션, 에포크, 학습률, 배치 크기
    - **종류**: 배치 경사 하강법(전체 데이터), 확률적 경사 하강법(1개 데이터), 미니배치 경사 하강법(적당량의 데이터)

---

## 2. 데이터 처리와 모델 평가

### 2.1. 데이터 탐색 (EDA) 및 시각화
- **상관 관계 (Correlation)**: 두 변수가 함께 변화하는 관련성. 인과관계를 의미하지 않음.
- **시각화 라이브러리**: Matplotlib (기본), Seaborn (통계적, 미학적)
- **그래프 종류**:
    - **히스토그램**: 단일 변수의 분포 확인.
    - **박스 플롯**: 사분위수를 이용해 데이터 분포와 이상치 탐지.
    - **산점도**: 두 변수 간의 관계 패턴 확인.
    - **히트맵**: 여러 변수 간의 상관관계를 색상으로 표현.
    - **페어 플롯**: 여러 변수 간의 관계를 한 번에 시각화.

### 2.2. 모델 학습 및 전처리
- **데이터 분리**: 훈련(train), 검증(validation), 테스트(test) 데이터로 분리하여 모델 성능을 객관적으로 평가.
- **과적합 (Overfitting)**: 모델이 훈련 데이터에만 너무 익숙해져 새로운 데이터에 대한 성능이 낮아지는 현상.
- **과소적합 (Underfitting)**: 모델이 너무 단순하여 훈련 데이터의 패턴조차 제대로 학습하지 못하는 현상.
- **데이터 스케일링**:
    - **표준화 (Standardization)**: 평균 0, 표준편차 1로 변환. 이상치에 덜 민감.
    - **정규화 (Normalization)**: 0과 1 사이의 값으로 변환. 이상치에 민감.

### 2.3. 모델 평가
- **회귀 평가**: MSE (평균 제곱 오차)
- **분류 평가**:
    - **정확도 (Accuracy)**: 전체 예측 중 올바르게 예측한 비율. 불균형 데이터에 취약.
    - **혼동 행렬 (Confusion Matrix)**: TP, TN, FP, FN을 통해 모델 성능을 상세히 분석.
    - **정밀도 (Precision)**: Positive로 예측한 것 중 실제 Positive의 비율.
    - **재현율 (Recall)**: 실제 Positive 중 모델이 Positive로 예측한 비율.
    - **F1-점수**: 정밀도와 재현율의 조화 평균.
- **성능 검증**: K-폴드 교차 검증 등을 통해 모델 성능 평가의 신뢰성을 높임.

---

## 3. 딥러닝 핵심

### 3.1. 신경망 기초
- **퍼셉트론**: 인공 신경망의 기본 단위 (입력, 가중치, 활성화 함수).
- **심층 신경망 (DNN)**: 여러 개의 은닉층을 깊게 쌓아 복잡한 패턴을 스스로 학습하는 모델.
- **MLP (다중 퍼셉트론)**: 퍼셉트론을 여러 층으로 쌓은 모델.
    - **학습 과정**: 순전파 → 손실 계산 → 역전파 → 가중치 업데이트

### 3.2. 학습 성능 향상 기법
- **활성화 함수**: ReLU (기울기 소실 문제 해결, 빠른 속도).
- **옵티마이저**: 손실 함수를 최소화하는 최적의 가중치를 찾는 알고리즘 (예: Adam).
- **규제 (Regularization)**: 과적합 방지.
    - **L1/L2 규제**: 가중치 크기에 제약을 줌.
    - **드롭아웃 (Dropout)**: 학습 시 일부 뉴런을 무작위로 비활성화.

---

## 4. 자연어 처리 (NLP)

### 4.1. 임베딩과 순차 데이터 처리
- **원-핫 인코딩**: 단어를 희소 벡터로 표현. 차원의 저주, 유사성 표현 불가 문제.
- **Word2Vec**: 단어의 의미를 밀집 벡터로 표현. 단어 간 의미적 관계 학습 가능.
- **RNN (순환 신경망)**: 순차적 데이터를 처리. 기울기 소실 문제로 장기 의존성 학습에 한계.
- **LSTM (장단기 메모리)**: 셀 상태와 게이트를 도입하여 RNN의 기울기 소실 문제를 해결.

### 4.2. 트랜스포머와 어텐션
- **Seq2Seq**: 인코더-디코더 구조로, 입력 시퀀스를 다른 시퀀스로 변환 (예: 번역). 컨텍스트 벡터의 병목(bottleneck) 현상 한계.
- **어텐션 (Attention)**: Seq2Seq의 병목 현상을 해결. 디코더가 출력할 때 입력 시퀀스의 중요한 부분에 더 집중.
- **셀프 어텐션 (Self-Attention)**: 트랜스포머의 핵심. 시퀀스 내 단어들 간의 관계를 병렬적으로 계산.
    - **구성**: Query, Key, Value 벡터
    - **한계**: 순서 정보 부재 → 위치 인코딩(Positional Encoding)으로 해결.
- **트랜스포머 (Transformer)**: 셀프 어텐션에 기반한 인코더-디코더 구조. RNN을 완전히 배제하여 병렬 처리 및 장거리 의존성 학습에 매우 효과적.
- **BERT**: 트랜스포머의 인코더 구조만 사용하여 문장의 양방향 문맥을 이해하는 데 특화된 모델.

---

## 5. 컴퓨터 비전 (CV)

### 5.1. CNN (합성곱 신경망)
- **합성곱 (Convolution)**: 필터를 이용해 이미지의 지역적 특징(모서리, 질감 등)을 추출.
- **풀링 (Pooling)**: 특징 맵의 크기를 줄여(다운샘플링) 계산 효율을 높이고 강건한 특징을 추출.
- **주요 아키텍처**:
    - **VGGNet**: 3x3의 작은 필터를 깊게 쌓아 성능 향상을 증명.
    - **ResNet**: 잔차 연결(스킵 연결)을 도입하여 매우 깊은 신경망의 학습을 가능하게 함.
    - **MobileNet**: 깊이별 분리 합성곱을 사용한 모바일 환경용 경량화 모델.
- **ViT (비전 트랜스포머)**: 이미지를 패치로 나누어 트랜스포머 구조로 처리. 전역적 문맥 이해에 강점.

---

## 6. 파운데이션 모델과 LLM

### 6.1. 핵심 개념
- **규모의 법칙 (Scaling Law)**: 모델/데이터/학습량이 커질수록 성능이 예측 가능하게 향상.
- **창발성 (Emergent Property)**: 특정 규모를 넘어서면서 예측하지 못했던 새로운 능력이 발현 (예: 인-컨텍스트 학습).
- **정렬 (Alignment)**: LLM의 출력을 인간의 의도와 가치에 부합하도록 조정하는 과정.
    - **RLHF (인간 피드백 기반 강화학습)**: 인간의 선호를 보상 모델로 학습시켜 LLM을 강화학습으로 파인튜닝하는 핵심적인 정렬 기법.

### 6.2. 주요 한계와 해결책
- **환각 (Hallucination)**: 사실과 다른 내용을 생성하는 문제.
    - **해결책**: **RAG (검색 증강 생성)** - 외부의 신뢰할 수 있는 정보를 검색하여 답변에 활용.
- **탈옥 (Jailbreaking)**: 안전 장치를 우회하여 유해한 콘텐츠를 생성하도록 유도하는 행위.

### 6.3. RAG (검색 증강 생성)
- **핵심 과정**: 파싱(Parsing) → 청킹(Chunking) → 인덱싱(Indexing) → 검색(Retrieval) → 생성(Generation).
- **검색기 (Retriever)**:
    - **Sparse Retriever**: 키워드 기반 검색 (예: TF-IDF, BM25).
    - **Dense Retriever**: 의미 기반 벡터 검색 (예: Bi-encoder, Cross-encoder).
    - **Hybrid Search**: 두 방식의 장점을 결합.

---

## 7. 모델 최적화 및 경량화

### 7.1. PEFT (파라미터 효율적 파인튜닝)
- **LoRA (Low-Rank Adaptation)**: 기존 가중치는 고정하고, 랭크가 낮은 작은 행렬을 추가하여 이 행렬만 학습하는 대표적인 PEFT 기법.
- **QLoRA**: LoRA와 양자화를 결합하여 메모리 사용량을 획기적으로 줄인 기술.

### 7.2. 모델 압축 기술
- **양자화 (Quantization)**: 가중치와 활성화 값의 정밀도(비트 수)를 낮춰 모델 크기와 연산량을 줄임 (예: PTQ, QAT).
- **가지치기 (Pruning)**: 중요도가 낮은 가중치 연결을 제거하여 모델의 파라미터 수를 줄임.
- **지식 증류 (Knowledge Distillation)**: 크고 복잡한 교사 모델의 지식을 작고 빠른 학생 모델에게 전달.