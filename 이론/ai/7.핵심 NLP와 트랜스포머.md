## 문제 1 (난이도: 보통)
순환 신경망(RNN)이 긴 시퀀스 데이터를 처리할 때, 과거의 정보가 뒤로 갈수록 점차 희미해져 장기 의존성(Long-Term Dependency) 학습이 어려워지는 핵심적인 한계는 무엇인가?

### 보기
A. 차원의 저주 (Curse of Dimensionality)
B. 기울기 소실 또는 폭주 문제 (Vanishing/Exploding Gradient Problem)
C. 과적합 문제 (Overfitting Problem)
D. 병목 현상 (Bottleneck Problem)
E. 계산 복잡성 증가 (Increased Computational Complexity)

**정답:** B
**해설:** RNN은 시퀀스를 처리하며 역전파 과정에서 동일한 가중치 행렬을 반복적으로 곱하게 됩니다. 이 과정에서 기울기가 1보다 작으면 계속 곱해져 0에 가까워지고(기울기 소실), 1보다 크면 무한대로 발산(기울기 폭주)할 수 있습니다. 특히 기울기 소실은 모델이 오래전 정보를 학습하지 못하게 만드는 RNN의 고질적인 문제입니다.
- A: 주로 원-핫 인코딩과 같이 벡터의 차원이 불필요하게 커질 때 발생합니다.
- C: 모델이 훈련 데이터에만 과도하게 최적화되는 현상입니다.
- D: Seq2Seq 모델에서 인코더의 정보를 고정된 크기의 벡터로 압축할 때 발생하는 정보 손실 문제입니다.
- E: RNN의 순차적 처리 방식은 병렬화가 어려워 계산 비효율성이 있지만, 장기 의존성 학습 실패의 직접적인 원인은 아닙니다.

---

## 문제 2 (난이도: 보통)
LSTM(Long Short-Term Memory)이 RNN의 장기 의존성 문제를 해결하기 위해 도입한 핵심적인 메커니즘으로, '정보를 얼마나 잊을지', '새로운 정보를 얼마나 저장할지', '어떤 값을 출력할지'를 결정하는 세 가지 요소는 무엇인가?

### 보기
A. 어텐션, 쿼리, 밸류 (Attention, Query, Value)
B. 인코더, 디코더, 컨텍스트 벡터 (Encoder, Decoder, Context Vector)
C. 커널, 스트라이드, 패딩 (Kernel, Stride, Padding)
D. 망각, 입력, 출력 게이트 (Forget, Input, Output Gates)
E. 순전파, 역전파, 손실 함수 (Forward/Back Propagation, Loss Function)

**정답:** D
**해설:** LSTM은 RNN에 셀 상태(Cell State)라는 정보 흐름 경로를 추가하고, 이 셀 상태를 제어하는 세 개의 게이트(Gate)를 도입했습니다. 망각 게이트(Forget Gate)는 과거 정보를 얼마나 유지할지, 입력 게이트(Input Gate)는 현재 정보를 얼마나 추가할지, 출력 게이트(Output Gate)는 셀 상태를 기반으로 어떤 값을 출력(은닉 상태)으로 내보낼지를 결정하여 기울기 소실 문제를 완화합니다.
- A: 어텐션 메커니즘의 구성 요소입니다.
- B: Seq2Seq 모델의 구성 요소입니다.
- C: CNN(합성곱 신경망)의 구성 요소입니다.
- E: 일반적인 딥러닝 모델의 학습 과정입니다.

---

## 문제 3 (난이도: 보통)
Seq2Seq 모델에서 입력 시퀀스의 모든 정보를 고정된 크기의 벡터 하나에 압축하면서 발생하는 정보 손실 문제(Bottleneck Problem)를 해결하기 위해 등장한 메커니즘은 무엇인가?

### 보기
A. 어텐션 (Attention)
B. 드롭아웃 (Dropout)
C. 배치 정규화 (Batch Normalization)
D. 잔여 연결 (Residual Connection)
E. 교차 검증 (Cross-Validation)

**정답:** A
**해설:** 어텐션 메커니즘은 디코더가 출력을 생성할 때마다 인코더의 모든 은닉 상태(입력 시퀀스의 모든 부분)를 다시 참조하여, 현재 예측에 가장 중요한 정보에 '집중'하도록 가중치를 부여합니다. 이를 통해 고정된 크기의 컨텍스트 벡터에만 의존하지 않고, 필요할 때마다 입력 시퀀스의 관련 정보를 직접 활용하여 병목 현상을 해결하고 성능을 크게 향상시켰습니다.
- B, C, D: 딥러닝 모델의 학습 안정화 및 성능 향상을 위한 다른 기법들입니다.
- E: 모델의 성능을 평가하는 기법입니다.

---

## 문제 4 (난이도: 보통)
트랜스포머(Transformer)의 핵심 구성 요소인 셀프 어텐션(Self-Attention)에서, 각 단어가 다른 단어로부터 정보를 찾기 위해 사용하는 벡터, 자신의 정보를 표현하는 벡터, 그리고 실제로 참조되는 정보 내용을 담고 있는 벡터를 순서대로 올바르게 짝지은 것은?

### 보기
A. Key, Value, Query
B. Value, Query, Key
C. Query, Key, Value
D. Query, Value, Key
E. Key, Query, Value

**정답:** C
**해설:** 셀프 어텐션은 하나의 시퀀스 내에서 단어 간의 관계를 계산하기 위해 각 단어에 대해 세 가지 벡터를 사용합니다.
- **Query:** 현재 단어가 다른 단어들과의 유사도를 계산하기 위해 사용하는 '질의' 벡터입니다.
- **Key:** 다른 단어들이 현재 Query에 대해 자신의 어떤 정보를 보여줄지를 결정하는 '키' 벡터입니다.
- **Value:** 유사도 계산 후, 실제로 가중합되어 현재 단어의 새로운 표현에 사용되는 '값' 벡터입니다.
Query는 Key와 유사도를 계산하고, 이 유사도를 가중치로 하여 Value를 가져온다고 이해할 수 있습니다.

---

## 문제 5 (난이도: 보통)
다음 중 RNN 대신 셀프 어텐션(Self-Attention)을 기반으로 하는 트랜스포머(Transformer) 모델의 주요 장점으로 가장 적절한 것은?

### 보기
A. 순차적인 데이터 처리 방식으로 인해 시간적 순서 정보를 자연스럽게 학습한다.
B. 모델의 크기가 작고 파라미터 수가 적어 계산 효율성이 매우 높다.
C. 입력 시퀀스의 길이에 제약이 없어 어떤 길이의 텍스트도 처리할 수 있다.
D. 재귀적 구조가 없어 병렬 처리가 가능하며, 시퀀스 내 단어 간의 직접적인 상호작용을 통해 장기 의존성 문제를 해결한다.
E. 별도의 위치 인코딩(Positional Encoding) 없이 단어의 순서를 완벽하게 이해한다.

**정답:** D
**해설:** 트랜스포머는 RNN의 재귀적(순차적) 구조를 완전히 배제하고 셀프 어텐션 메커니즘을 사용합니다. 이로 인해 한 번에 모든 단어를 처리하는 병렬 계산이 가능해져 학습 속도가 크게 향상되었습니다. 또한, 셀프 어텐션은 시퀀스 내의 모든 단어 쌍 간의 관계를 직접 계산하므로(최대 상호작용 거리=1), RNN의 기울기 소실 문제 없이 장기 의존성을 효과적으로 학습할 수 있습니다.
- A: RNN의 특징이며, 트랜스포머는 이 구조를 버렸습니다.
- B: 트랜스포머 기반 모델(LLM)은 일반적으로 매우 큽니다.
- C: 기술적으로는 가능하지만, 실제로는 메모리 및 계산 비용 문제로 입력 길이에 제약이 있습니다.
- E: 셀프 어텐션 자체는 순서 정보가 없으므로, 위치 인코딩을 별도로 추가하여 순서 정보를 주입해야 합니다.
