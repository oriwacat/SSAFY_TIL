## 문제 1 (난이도: 쉬움)
회귀(Regression) 모델의 성능을 평가하는 가장 대표적인 지표 중 하나로, 예측값과 실제값의 차이를 제곱하여 평균 낸 값은 무엇인가?

### 보기
A. 정확도 (Accuracy)
B. 정밀도 (Precision)
C. 재현율 (Recall)
D. 평균 제곱 오차 (MSE)
E. F1-점수 (F1-Score)

**정답:** D
**해설:** 평균 제곱 오차(Mean Squared Error, MSE)는 회귀 모델의 예측 오차를 측정하는 데 사용되는 지표입니다. 예측값과 실제값의 차이를 제곱하여 평균을 내므로, 오차가 클수록 더 큰 페널티를 부여합니다.
- A, B, C, E는 주로 분류 모델의 성능 평가에 사용되는 지표입니다.

---

## 문제 2 (난이도: 쉬움)
분류 모델의 성능 평가 지표 중, 모델이 "Positive"라고 예측한 것들 중에서 실제로 "Positive"인 비율을 나타내는 것은 무엇인가?

### 보기
A. 재현율 (Recall)
B. 정밀도 (Precision)
C. 정확도 (Accuracy)
D. F1-점수 (F1-Score)
E. 특이도 (Specificity)

**정답:** B
**해설:** 정밀도(Precision)는 모델이 긍정(Positive)으로 예측한 결과 중 실제 긍정인 비율을 나타냅니다. 이는 모델이 얼마나 정교하게 긍정 클래스를 예측하는지를 보여줍니다.
- A: 실제 긍정 중 모델이 긍정으로 예측한 비율입니다.
- C: 전체 예측 중 맞은 예측의 비율입니다.
- D: 정밀도와 재현율의 조화 평균입니다.
- E: 실제 부정 중 모델이 부정으로 예측한 비율입니다.

---

## 문제 3 (난이도: 보통)
다음 혼동 행렬(Confusion Matrix)에 대한 설명 중 옳지 않은 것은?

### 보기
A. TP(True Positive)는 실제 Positive를 Positive로 올바르게 예측한 경우이다.
B. TN(True Negative)은 실제 Negative를 Negative로 올바르게 예측한 경우이다.
C. FP(False Positive)는 실제 Negative를 Positive로 잘못 예측한 경우로, 1종 오류(Type I Error)에 해당한다.
D. FN(False Negative)은 실제 Positive를 Negative로 잘못 예측한 경우로, 2종 오류(Type II Error)에 해당한다.
E. 혼동 행렬은 불균형 데이터셋에서 모델의 성능을 평가할 때 정확도(Accuracy)만으로 충분하다는 것을 보여준다.

**정답:** E
**해설:** 혼동 행렬은 분류 모델의 성능을 상세하게 분석하는 데 매우 유용하지만, 불균형 데이터셋에서는 정확도만으로는 모델의 성능을 제대로 평가하기 어렵습니다. 이 경우 정밀도, 재현율, F1-점수 등 다른 지표들을 함께 고려해야 합니다.
- A, B, C, D는 혼동 행렬의 구성 요소에 대한 올바른 설명입니다.

---

## 문제 4 (난이도: 보통)
모델의 성능 검증 과정에서 훈련 데이터를 다시 훈련 데이터와 검증 데이터로 나누어 사용하는 주된 이유는 무엇인가?

### 보기
A. 테스트 데이터의 양을 늘려 모델의 일반화 성능을 향상시키기 위해
B. 모델 학습 중 과적합 여부를 실시간으로 모니터링하고 하이퍼파라미터를 튜닝하기 위해
C. 훈련 데이터의 크기를 줄여 학습 시간을 단축하기 위해
D. 모델이 훈련 데이터에만 너무 익숙해지는 것을 방지하기 위해
E. 최종 모델 평가 시 테스트 데이터의 통계량을 활용하기 위해

**정답:** B
**해설:** 훈련 데이터를 다시 훈련/검증 데이터로 나누는 것은 모델 학습 과정에서 모델이 훈련 데이터에 과적합되는지 여부를 확인하고, 이에 따라 하이퍼파라미터(예: 학습률, 규제 강도)를 조정하는 데 사용됩니다. 검증 데이터는 모델 학습에는 직접 사용되지 않지만, 모델의 성능을 평가하고 최적의 모델을 선택하는 데 중요한 역할을 합니다.
- A, C, D, E는 주된 이유가 아닙니다.

---

## 문제 5 (난이도: 쉬움)
인공 신경망의 가장 기본적인 단위로, 입력값에 가중치를 곱하고 활성화 함수를 통과시켜 출력값을 내는 구조는 무엇인가?

### 보기
A. 은닉층 (Hidden Layer)
B. 출력층 (Output Layer)
C. 퍼셉트론 (Perceptron)
D. 컨볼루션 필터 (Convolutional Filter)
E. 순환 신경망 (Recurrent Neural Network)

**정답:** C
**해설:** 퍼셉트론은 인공 신경망의 기본 구성 요소로, 여러 개의 입력을 받아 가중치와 곱한 후 합산하여 활성화 함수를 통해 최종 출력값을 생성합니다. 이는 인간 뇌의 뉴런을 모방한 초기 모델입니다.
- A, B는 신경망의 계층을 의미합니다.
- D는 CNN에서 사용되는 요소입니다.
- E는 특정 구조의 신경망입니다.

---

## 문제 6 (난이도: 보통)
MLP(다중 퍼셉트론)의 학습 과정 중, 모델의 예측값과 실제 정답값의 차이를 계산하는 단계는 무엇인가?

### 보기
A. 순전파 (Forward Propagation)
B. 역전파 (Back Propagation)
C. 가중치 업데이트 (Weight Update)
D. 손실 함수 계산 (Loss Function Calculation)
E. 활성화 함수 적용 (Activation Function Application)

**정답:** D
**해설:** 손실 함수(Loss Function)는 모델의 예측값과 실제 정답값 사이의 오차(또는 불일치 정도)를 정량적으로 측정하는 함수입니다. 이 손실 값을 최소화하는 방향으로 모델이 학습됩니다.
- A: 입력이 출력으로 전달되는 과정입니다.
- B: 손실을 기반으로 가중치를 업데이트하기 위해 기울기를 계산하는 과정입니다.
- C: 역전파를 통해 계산된 기울기를 사용하여 가중치를 조정하는 과정입니다.
- E: 퍼셉트론의 출력값을 비선형적으로 변환하는 함수입니다.

---

## 문제 7 (난이도: 보통)
딥러닝 모델 학습 시 과적합을 방지하기 위한 규제(Regularization) 기법 중, 일부 가중치를 0으로 만들어 모델의 복잡도를 감소시키고 불필요한 특성을 제거하는 효과가 있는 것은 무엇인가?

### 보기
A. L2 규제 (L2 Regularization)
B. 드롭아웃 (Dropout)
C. L1 규제 (L1 Regularization)
D. 배치 정규화 (Batch Normalization)
E. 조기 종료 (Early Stopping)

**정답:** C
**해설:** L1 규제(Lasso Regularization)는 손실 함수에 가중치 절댓값의 합을 추가하여, 학습 과정에서 일부 가중치를 0으로 만듭니다. 이는 모델의 특성 선택(feature selection) 효과를 가져와 모델을 더 간결하게 만들고 과적합을 방지합니다.
- A: 가중치를 0에 가깝게 만들지만 완전히 0으로 만들지는 않습니다.
- B: 학습 시 뉴런을 무작위로 비활성화하여 과적합을 방지합니다.
- D: 학습 속도를 향상시키고 과적합을 완화하는 데 도움을 줍니다.
- E: 검증 손실이 증가하기 시작할 때 학습을 중단하여 과적합을 방지합니다.

----- 

## 문제 8 (난이도: 쉬움)
딥러닝 모델의 학습 성능 향상을 위해 사용되는 활성화 함수 중, 0보다 작으면 0을 출력하고 0보다 크면 입력값을 그대로 출력하여 기울기 소실 문제를 완화하고 계산 속도가 빠른 것은 무엇인가?

### 보기
A. 시그모이드 (Sigmoid)
B. 하이퍼볼릭 탄젠트 (Tanh)
C. 소프트맥스 (Softmax)
D. ReLU (Rectified Linear Unit)
E. Leaky ReLU

**정답:** D
**해설:** ReLU(Rectified Linear Unit)는 0 이하의 값은 0으로, 0 초과의 값은 그대로 출력하는 활성화 함수입니다. 이는 시그모이드나 Tanh 함수에서 발생할 수 있는 기울기 소실(vanishing gradient) 문제를 완화하고, 계산이 단순하여 학습 속도를 빠르게 하는 장점이 있습니다.
- A, B는 기울기 소실 문제가 발생할 수 있습니다.
- C는 주로 다중 클래스 분류 문제의 출력층에서 사용됩니다.
- E는 ReLU의 변형으로, 0 이하의 값에 작은 양의 기울기를 부여합니다.

---

## 문제 9 (난이도: 보통)
딥러닝 모델 학습 시 손실 함수의 값이 최소가 되는 최적의 가중치를 찾기 위한 알고리즘을 무엇이라고 하는가? 대표적으로 경사 하강법(Gradient Descent)이 있으며, 최근에는 Adam과 같은 방식이 많이 사용된다.

### 보기
A. 활성화 함수 (Activation Function)
B. 손실 함수 (Loss Function)
C. 옵티마이저 (Optimizer)
D. 규제 (Regularization)
E. 배치 정규화 (Batch Normalization)

**정답:** C
**해설:** 옵티마이저(Optimizer)는 손실 함수를 최소화하기 위해 모델의 가중치와 편향을 업데이트하는 방법을 정의하는 알고리즘입니다. 경사 하강법의 다양한 변형(SGD, Adam, RMSprop 등)이 여기에 해당합니다.
- A: 뉴런의 출력값을 결정합니다.
- B: 모델의 예측 오차를 측정합니다.
- D: 과적합을 방지하기 위한 기법입니다.
- E: 학습 속도와 안정성을 향상시킵니다.

---

## 문제 10 (난이도: 보통)
K-Fold 교차 검증(Cross-Validation)에 대한 설명으로 가장 적절한 것은?

### 보기
A. 데이터를 K개의 동일한 크기의 폴드(fold)로 나눈 후, K-1개의 폴드를 훈련 데이터로, 나머지 1개의 폴드를 테스트 데이터로 사용하여 모델을 평가한다.
B. 훈련 데이터를 K개의 폴드로 나눈 뒤, 각 폴드가 한 번씩 검증 데이터 역할을 하도록 하여 모델을 K번 학습하고 평가한다.
C. K-Fold 교차 검증은 모델의 과적합을 직접적으로 해결하는 가장 효과적인 방법이다.
D. 주로 회귀 모델의 성능 평가에만 사용되며, 분류 모델에는 적합하지 않다.
E. K의 값이 클수록 모델 학습 시간이 단축되는 장점이 있다.

**정답:** B
**해설:** K-Fold 교차 검증은 훈련 데이터를 K개의 폴드로 나누고, 각 폴드를 한 번씩 검증 데이터로 사용하여 모델을 K번 학습하고 평가하는 방법입니다. 이를 통해 데이터 분할에 따른 모델 성능의 편향을 줄이고, 모델의 일반화 성능을 더 신뢰성 있게 평가할 수 있습니다.
- A: K-Fold의 일반적인 설명이 아닙니다.
- C: 과적합을 직접 해결하기보다는 모델의 일반화 성능을 더 잘 평가하는 데 목적이 있습니다.
- D: 분류 모델에도 널리 사용됩니다.
- E: K의 값이 클수록 모델 학습 시간이 길어집니다.

---

## 문제 11 (난이도: 보통)
딥러닝 모델 학습 시, 학습률(learning rate)이 너무 높을 때 발생할 수 있는 문제점은?

### 보기
A. 모델이 지역 최저점(local minimum)에 빠져 학습이 정체된다.
B. 손실 함수가 발산하거나 최적점을 지나쳐 버리는 오버슈팅(overshooting)이 발생할 수 있다.
C. 학습 시간이 지나치게 길어져 비효율적이다.
D. 모델이 훈련 데이터에 과소적합(underfitting)될 가능성이 높아진다.
E. 가중치 업데이트가 너무 미미하여 학습이 거의 진행되지 않는다.

**정답:** B
**해설:** 학습률이 너무 높으면 옵티마이저가 손실 함수의 최저점을 찾아가는 과정에서 너무 큰 폭으로 이동하여 최적점을 지나치거나, 심지어 손실 함수가 발산하여 학습이 제대로 이루어지지 않을 수 있습니다. 이를 오버슈팅이라고 합니다.
- A, C, D, E는 학습률이 너무 낮을 때 발생할 수 있는 문제점 또는 관련 없는 설명입니다.

---

## 문제 12 (난이도: 보통)
MLP(다중 퍼셉트론)의 학습 과정 중, 손실 값을 기반으로 출력층에서 입력층 방향으로 거꾸로 계산하면서 각 계층의 가중치를 업데이트하는 데 필요한 기울기(gradient)를 계산하는 과정은 무엇인가?

### 보기
A. 순전파 (Forward Propagation)
B. 역전파 (Back Propagation)
C. 가중치 초기화 (Weight Initialization)
D. 활성화 함수 (Activation Function)
E. 손실 함수 (Loss Function)

**정답:** B
**해설:** 역전파(Back Propagation)는 순전파를 통해 계산된 손실 값을 이용하여 출력층에서부터 입력층 방향으로 각 계층의 가중치에 대한 손실 함수의 기울기를 계산하는 알고리즘입니다. 이 기울기를 사용하여 가중치를 업데이트함으로써 모델의 성능을 개선합니다.
- A: 입력이 출력으로 전달되는 과정입니다.
- C: 학습 시작 전 가중치에 초기값을 부여하는 것입니다.
- D: 뉴런의 출력값을 결정합니다.
- E: 모델의 예측 오차를 측정합니다.

---

## 문제 13 (난이도: 보통)
다음 중 딥러닝 모델에서 L2 규제(L2 Regularization)를 사용하는 주된 목적은?

### 보기
A. 모델의 학습 속도를 가속화하기 위해
B. 특정 특성(feature)의 가중치를 완전히 0으로 만들어 특성 선택을 수행하기 위해
C. 가중치 값이 너무 커지는 것을 방지하여 모델의 과적합을 완화하기 위해
D. 신경망의 각 뉴런을 무작위로 비활성화하여 편향 학습을 방지하기 위해
E. 데이터의 스케일을 조정하여 모델의 안정성을 높이기 위해

**정답:** C
**해설:** L2 규제(Ridge Regularization)는 손실 함수에 가중치 제곱의 합을 추가하여, 가중치 값이 너무 커지는 것을 제약합니다. 이는 모델의 복잡도를 줄이고 과적합을 완화하는 데 도움을 줍니다. 가중치를 0에 가깝게 만들지만 완전히 0으로 만들지는 않습니다.
- B는 L1 규제의 특징입니다.
- D는 드롭아웃의 특징입니다.
- A, E는 L2 규제의 직접적인 목적이 아닙니다.

---

## 문제 14 (난이도: 보통)
최근 대규모 언어 모델(LLM)의 파인튜닝(Fine-tuning) 과정에서, 훈련 데이터에 대한 손실은 계속 감소하지만 검증 데이터에 대한 성능이 더 이상 개선되지 않거나 오히려 나빠질 때 학습을 중단하는 기법은 무엇인가? (2024년 AI 트렌드 참고)

### 보기
A. 데이터 증강 (Data Augmentation)
B. 배치 정규화 (Batch Normalization)
C. 조기 종료 (Early Stopping)
D. 전이 학습 (Transfer Learning)
E. 학습률 스케줄링 (Learning Rate Scheduling)

**정답:** C
**해설:** 조기 종료(Early Stopping)는 딥러닝 모델의 과적합을 방지하는 효과적인 기법 중 하나입니다. 훈련 데이터에 대한 손실은 계속 감소하더라도, 검증 데이터에 대한 성능이 일정 에포크 동안 개선되지 않거나 악화되기 시작하면 학습을 중단하여 과적합된 모델을 저장하는 것을 방지합니다.
- A, B, D, E는 과적합 방지 또는 학습 성능 향상에 기여할 수 있지만, 직접적으로 검증 성능을 모니터링하여 학습을 중단하는 기법은 아닙니다.

---

## 문제 15 (난이도: 보통)
딥러닝 모델 학습 시, 드롭아웃(Dropout) 기법을 사용하는 주된 목적은?

### 보기
A. 모델의 학습 속도를 빠르게 하기 위해
B. 각 뉴런의 출력값을 0과 1 사이로 정규화하기 위해
C. 모델이 특정 뉴런이나 특성에 과도하게 의존하는 것을 방지하고 과적합을 완화하기 위해
D. 손실 함수의 최저점을 더 효율적으로 찾기 위해
E. 입력 데이터의 차원을 축소하여 모델의 복잡도를 줄이기 위해

**정답:** C
**해설:** 드롭아웃은 훈련 과정에서 각 학습 단계마다 무작위로 일부 뉴런을 비활성화(출력을 0으로 설정)하는 기법입니다. 이는 모델이 특정 뉴런에 과도하게 의존하는 것을 방지하고, 여러 개의 작은 모델을 앙상블하는 효과를 내어 과적합을 완화하는 데 매우 효과적입니다.
- A, B, D, E는 드롭아웃의 직접적인 목적이 아닙니다.
