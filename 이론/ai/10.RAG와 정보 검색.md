## 문제 1 (난이도: 보통)
RAG(검색 증강 생성) 파이프라인에서 긴 문서를 처리할 때, `RecursiveCharacterTextSplitter`를 사용하여 텍스트를 여러 개의 청크(Chunk)로 분할한다. 이때 `chunk_overlap` 파라미터를 설정하는 가장 주된 이유는 무엇인가?

### 보기
A. 각 청크의 크기를 정확히 `chunk_size`에 맞추기 위해
B. 청크의 총 개수를 줄여 임베딩 비용을 절약하기 위해
C. 인접한 청크 간의 의미적 연속성을 유지하고, 문맥이 잘리는 것을 방지하기 위해
D. 문서에 있는 불필요한 구분자(separator)를 제거하기 위해
E. 각 청크가 서로 다른 의미를 갖도록 강제하여 검색 다양성을 높이기 위해

**정답:** C
**해설:** `chunk_overlap`은 인접한 청크 사이에 텍스트 일부를 겹치게 만드는 역할을 합니다. 만약 겹치는 부분이 없다면, 중요한 의미를 가진 문장이 두 청크의 경계에서 정확히 잘려나가 검색 시 해당 문맥을 완전히 놓칠 수 있습니다. 중첩을 통해 이러한 정보 손실을 방지하고, 어떤 질문이 들어와도 관련된 전체 문맥이 하나의 청크에 포함될 확률을 높여줍니다.
- A: `chunk_size`는 최대 크기를 의미하며, `chunk_overlap`은 크기를 맞추는 목적이 아닙니다.
- B: 오히려 전체 토큰 수가 약간 늘어날 수 있습니다.
- D: 구분자는 `separators` 파라미터로 관리됩니다.
- E: 반대로, 의미적 연속성을 유지하는 것이 목적입니다.

---

## 문제 2 (난이도: 보통)
정보 검색(Information Retrieval) 시스템에서 사용되는 Sparse Retriever와 Dense Retriever에 대한 설명으로 옳지 않은 것은?

### 보기
A. Sparse Retriever는 TF-IDF와 같이 키워드 일치 기반으로 동작하며, 계산이 빠르다.
B. Dense Retriever는 임베딩을 활용하여 단어의 의미적 유사성을 기반으로 검색한다.
C. Sparse Retriever는 동의어나 유의어 관계를 이해하지 못하는 한계가 있다.
D. Dense Retriever는 항상 Sparse Retriever보다 검색 정확도가 뛰어나므로, 현대 RAG 시스템에서는 Sparse Retriever를 사용하지 않는다.
E. 하이브리드 검색(Hybrid Search)은 두 방식의 장점을 결합하여 검색 성능을 높이는 접근법이다.

**정답:** D
**해설:** Dense Retriever가 의미적 검색에 강점이 있지만, 특정 키워드나 고유명사가 반드시 포함되어야 하는 검색에서는 키워드 기반의 Sparse Retriever가 더 좋은 성능을 보일 때가 많습니다. 따라서 현대의 많은 고성능 RAG 시스템은 두 방식의 결과를 모두 활용하는 하이브리드 검색을 채택하여, 의미적 관련성과 키워드 정확성을 모두 잡으려고 합니다.
- A, B, C, E는 모두 각 리트리버의 특징과 최신 동향을 올바르게 설명하고 있습니다.

---

## 문제 3 (난이도: 보통)
Dense Retriever에서 사용되는 Bi-encoder와 Cross-encoder 아키텍처에 대한 설명으로 가장 적절한 것은?

### 보기
A. Bi-encoder는 질문(Query)과 문서(Document)를 함께 입력받아 한 번에 관련성을 계산하므로 정확도가 높다.
B. Cross-encoder는 질문과 문서를 각각 독립적으로 임베딩한 후 벡터 유사도를 계산하여 속도가 매우 빠르다.
C. Bi-encoder는 속도가 빠른 장점 때문에 RAG의 1단계 후보군 필터링에, Cross-encoder는 정확도가 높은 장점 때문에 2단계 재순위(re-ranking)에 주로 사용된다.
D. Cross-encoder는 구조상 사전 학습이 불가능하여 실시간으로만 학습해야 한다.
E. Bi-encoder는 질문과 문서 간의 상호작용을 직접 모델링하기 때문에 미묘한 문맥을 잘 파악한다.

**정답:** C
**해설:** 두 아키텍처는 속도와 정확도의 트레이드오프 관계를 가집니다. Bi-encoder는 질문과 문서를 미리 각각 임베딩해둘 수 있어 검색 속도가 매우 빠르지만, 둘 사이의 상호작용을 직접 보지 못해 정확도에 한계가 있습니다. 반면, Cross-encoder는 둘을 묶어서 처리하므로 상호작용을 깊게 이해해 정확도가 높지만, 검색 시마다 모든 후보 문서와 질문을 쌍으로 모델에 통과시켜야 하므로 매우 느립니다. 따라서 1단계에서 Bi-encoder로 후보를 빠르게 추리고, 2단계에서 Cross-encoder로 소수의 후보군 순위를 정교하게 다시 매기는 방식이 효과적입니다.
- A, B, E는 Bi-encoder와 Cross-encoder의 특징을 서로 반대로 설명하고 있습니다.
- D는 사실과 다릅니다.

---

## 문제 4 (난이도: 보통)
RLHF(인간 피드백 기반 강화학습)는 LLM의 응답을 인간의 의도에 맞게 '정렬(Alignment)'하는 핵심 기술이다. 하지만 RLHF 과정에서 모델이 보상 모델(Reward Model)의 허점을 파고들어, 실제로는 정답이 아니거나 품질이 낮음에도 불구하고 단지 보상 점수만 높게 받는 방향으로 응답을 생성하는 현상을 무엇이라고 하는가?

### 보기
A. 환각 (Hallucination)
B. 탈옥 (Jailbreaking)
C. 보상 해킹 (Reward Hacking)
D. 지시 따르기 (Instruction Following)
E. 자기 선호 편향 (Self-preference Bias)

**정답:** C
**해설:** 보상 해킹(Reward Hacking)은 강화학습 에이전트가 주어진 보상 함수의 허점을 이용하여 설계자가 의도하지 않은 방식으로 보상을 극대화하는 행동을 학습하는 현상을 의미합니다. RLHF에서는 LLM이 인간의 실제 선호나 답변의 품질과 무관하게, 단지 학습된 보상 모델로부터 높은 점수를 얻어내는 '꼼수'를 배우는 것으로 나타날 수 있습니다. 예를 들어, 무조건 길고 그럴듯하게 답변하는 경향을 보일 수 있습니다.
- A: 사실과 다른 내용을 생성하는 현상 자체를 의미합니다.
- B: 모델의 안전 지침을 우회하는 행위입니다.
- D: 정렬의 목표 중 하나입니다.
- E: LLM 평가 시 나타나는 편향의 한 종류입니다.

---

## 문제 5 (난이도: 보통)
다음은 RAG 시스템의 일반적인 처리 순서를 나타낸 것이다. 빈칸에 들어갈 가장 적절한 과정을 순서대로 짝지은 것은?

[원본 문서] → ( ① ) → [정제된 텍스트] → ( ② ) → [텍스트 청크 리스트] → ( ③ ) → [Vector Store]

### 보기
A. ① 파싱(Parsing), ② 청킹(Chunking), ③ 인덱싱(Indexing)
B. ① 청킹(Chunking), ② 인덱싱(Indexing), ③ 파싱(Parsing)
C. ① 인덱싱(Indexing), ② 파싱(Parsing), ③ 청킹(Chunking)
D. ① 파싱(Parsing), ② 인덱싱(Indexing), ③ 청킹(Chunking)
E. ① 청킹(Chunking), ② 파싱(Parsing), ③ 인덱싱(Indexing)

**정답:** A
**해설:** RAG의 데이터 준비(Ingestion) 과정은 다음과 같은 순서로 진행됩니다.
1.  **파싱(Parsing):** PDF, TXT, HTML 등 다양한 형태의 원본 문서에서 텍스트와 메타데이터를 추출합니다. (예: `PDFLoader`)
2.  **청킹(Chunking):** 추출된 긴 텍스트를 LLM이 처리하기 좋은 작은 단위(청크)로 분할합니다. (예: `RecursiveCharacterTextSplitter`)
3.  **인덱싱(Indexing):** 분할된 각 텍스트 청크를 임베딩 모델을 통해 벡터로 변환하고, 이 벡터를 나중에 빠르게 검색할 수 있도록 벡터 데이터베이스(Vector Store)에 저장합니다.

---

## 문제 6 (난이도: 쉬움)
LangChain과 같은 프레임워크가 LLM 기반 애플리케이션 개발에서 제공하는 핵심적인 역할로 가장 적절한 것은?

### 보기
A. 자체적으로 LLM을 처음부터 학습시키는 기능
B. 벡터 데이터베이스(Vector DB)를 대체하는 저장소 기능
C. 프롬프트, 모델, 외부 도구(Tool), 검색기(Retriever) 등을 연결하여 복잡한 워크플로우를 구성하는 파이프라인(Pipeline) 역할
D. 사용자의 프롬프트를 자동으로 최적화하여 항상 최고의 결과를 보장하는 기능
E. 생성된 텍스트의 유해성을 실시간으로 탐지하고 필터링하는 보안 기능

**정답:** C
**해설:** LangChain의 핵심 가치는 LLM을 단독으로 사용하는 것을 넘어, 다양한 구성 요소(컴포넌트)들을 마치 레고 블록처럼 조립하여 복잡한 애플리케이션을 쉽게 만들 수 있도록 돕는 '프레임워크' 또는 '파이프라인' 역할을 하는 데 있습니다. 예를 들어, RAG 시스템을 구축할 때 프롬프트 템플릿, LLM 모델, 벡터 DB 검색기, 출력 파서 등을 체인(Chain)으로 묶어 전체 과정을 자동화할 수 있습니다.
- A, B, D, E는 LangChain의 직접적인 핵심 역할이라기보다는, LangChain을 통해 구현할 수 있는 기능의 일부이거나 LangChain이 제공하지 않는 기능입니다.

---

## 문제 7 (난이도: 보통)
키워드 기반의 Sparse Retriever에서, 특정 단어가 한 문서 내에서는 자주 등장하지만 전체 문서 집합(Corpus)에서는 드물게 나타날수록 그 단어의 중요도가 높다고 판단하는 가중치 계산 방식은 무엇인가?

### 보기
A. 코사인 유사도 (Cosine Similarity)
B. 유클리드 거리 (Euclidean Distance)
C. TF-IDF (Term Frequency-Inverse Document Frequency)
D. BM25
E. 페이지랭크 (PageRank)

**정답:** C
**해설:** TF-IDF는 두 가지 지표의 곱으로 단어의 중요도를 계산합니다.
- **TF (Term Frequency):** 특정 문서 내에서 단어가 얼마나 자주 등장하는지. (높을수록 중요)
- **IDF (Inverse Document Frequency):** 그 단어가 전체 문서들 중에서 얼마나 희귀한지. (희귀할수록 중요)
따라서 'a', 'the'와 같이 모든 문서에 자주 나오는 단어는 낮은 IDF 값 때문에 중요도가 낮아지고, 특정 주제의 문서에만 집중적으로 나타나는 전문 용어는 높은 IDF 값 때문에 중요도가 높아집니다.
- A, B: 벡터 간의 유사도나 거리를 측정하는 방식입니다.
- D: TF-IDF를 개선한 또 다른 Sparse Retrieval 알고리즘입니다.
- E: 웹페이지의 중요도를 측정하는 알고리즘입니다.
